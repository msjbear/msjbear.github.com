<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[京东JData算法大赛小结(公司内部赛)]]></title>
    <url>%2F2017%2F06%2F10%2Fjdata%2F</url>
    <content type="text"><![CDATA[总体解决方案本文将高潜用户购买意向预测，抽象为一个二分类问题。从用户，商品，品牌，用户-商品，用户-品牌五个维度进行特征提取。将观察天未来5天有购买行为的用户-商品对标记为正样本，观察天过去30天至未来5天有交互行为但未购买的用户-商品对标记为负样本。由于正负样本比例极不平衡，采用了对正样本进行重采样及负样本进行下采样的方式来平衡正负样本比例。利用xgboost进行模型训练，最后利用LR对预测结果进行加权。取每个用户最高预测概率对应的user-sku对，取top12000作为最终输出结果。 实现方案技术栈集市堡垒机(环境) + Hive(ETL) + Spark&amp; Spark-Xgboost(Model) 特征工程从用户、商品、品牌、用户-商品交互、用户-品牌交互5个维度，再对各个维度从不同周期(1/3/5/7/10/15/30天)进行建模及特征提取。 3-1 用户维度(script/dim_feature_v3/dim_user_feature_etl.sql)：用户等级，性别，注册天数，年龄等级，浏览量，点击量，关注量，加购车量，下单量，取消关注量，点击购买率，点击加购率，点击关注率，浏览购买转化率，加购购买转化率，关注购买转化率，浏览day/sku/brand数，点击day/sku/brand数，关注day/sku/brand数，加购车day/sku/brand数，下单day/sku/brand数，取消关注day/sku/brand数，最近浏览/点击/购买/关注距今天数，平均每天浏览/点击/购买/关注量(sku/brand数)，平均浏览/点击/购买/关注行为操作间隔天数 3-2 商品维度(script/dim_feature_v3/dim_sku_feature_etl.sql)：商品评论数，好评率，差评率，商品属性1，属性2，属性3，浏览量，点击量，关注量，加购车量，下单量，取消关注量，点击购买率，点击加购率，点击关注率，浏览购买转化率，加购购买转化率，关注购买转化率，点击购买用户占比，点击加购用户占比，点击关注用户占比，浏览购买转化用户占比，加购购买转化用户占比，关注购买转化用户占比，浏览用户数，点击用户数，关注用户数，加购车用户数，下单用户数，取消关注用户数，平均每天浏览/点击/购买/关注量(用户数)，平均每个用户浏览/点击/购买/关注量 3-3 品牌维度(script/dim_feature_v3/dim_brand_feature_etl.sql)：浏览量，点击量，关注量，加购车量，下单量，取消关注量，点击购买率，点击加购率，点击关注率，浏览购买转化率，加购购买转化率，关注购买转化率，点击购买用户占比，点击加购用户占比，点击关注用户占比，浏览购买转化用户占比，加购购买转化用户占比，关注购买转化用户占比，浏览用户数，点击用户数，关注用户数，加购车用户数，下单用户数，取消关注用户数，平均每天浏览/点击/购买/关注量(用户数)，平均每个用户浏览/点击/购买/关注量，商品热度(点击量0.01+下单量0.5+加购量0.1-取消关注量0.1+关注量*0.1) 3-4 用户-商品交互维度(script/dim_feature_v3/dim_user_sku_feature_etl.sql)：浏览量，点击量，关注量，加购车量，下单量，取消关注量，点击购买率，点击加购率，点击关注率，浏览购买转化率，加购购买转化率，关注购买转化率，浏览day数，点击day数，关注day数，加购车day数，下单day数，取消关注day数，最近浏览/点击/购买/关注距今天数，平均浏览/点击/购买/关注行为操作间隔天数 3-5 用户品牌交互维度(script/dim_feature_v3/dim_user_brand_feature_etl.sql)：浏览量，点击量，关注量，加购车量，下单量，取消关注量，点击购买率，点击加购率，点击关注率，浏览购买转化率，加购购买转化率，关注购买转化率，浏览day数，点击day数，关注day数，加购车day数，下单day数，取消关注day数，最近浏览/点击/购买/关注距今天数，平均浏览/点击/购买/关注行为操作间隔天数 3-6 交叉类特征(script/dim_feature_v3/feature_wide_table.sql)：用户-商品与用户浏览/点击/关注/加购/下单/取消关注占比，用户-品牌与用户浏览/点击/关注/加购/下单/取消关注占比etc. 样本选择及特征预处理4-1 将观察天未来5天有购买行为的用户-商品对标记为正样本，观察天过去30天至未来5天有交互行为但未购买的用户-商品对标记为负样本。Eg.(将2016-04-06~2016-04-10有下单的用户-商品标记为正样本，将2016-03-06~2016-04-10有交互但未下单的用户-商品标记为负样本)。 4-2 由于正负样本比例极不平衡，正负样本比例约为1500:2400000，采用了对正样本进行重采样及负样本进行下采样的方式来平衡正负样本比例。具体采样方式为：将正样本复制10份，同时将负样本通过随机采样为200000 4-3 特征预处理：对于类型型特征：通过Spark VectorIndexer进行one-hot编码。对于连续型特征，通过Spark Normalizer进行归一化。 模型集成方案利用spark-xgboost进行滑窗模型训练，最后利用LR对各个预测结果进行加权。取每个用户最高预测概率对应的user-sku对，取top 12000作为最终输出结果。单模型准确率xgboost&gt;gbdt&gt;rf，因此只选择了xgboost以滑窗方式进行模型训练，未采用多模型融合stacking，融合过程也只用了lr加权融合，其它方式待尝试。 代码运行说明工程代码目录：JData-Spark使用maven进行源码编译，成功编译后会生成jdata-spark-1.0-SNAPSHOT-assembly.zipjdata-spark-1.0-SNAPSHOT-assembly.zip目录结构如下： 脚本运行进入到shell子目录，目录下各步骤脚本如下： 各个步骤运行说明，各个步骤间有依赖，每个步骤运行完再运行下一个步骤【首先需要将比赛数据集解压后手动放到data目录下】：6-2-1 step1_load_basic_data_and_create_wide_table.sh：加载数据集到hive表，并将user/product/action汇总加工成为一张基础数据宽表。运行命令：nohup sh step1_load_basic_data_and_create_wide_table.sh &gt; tmp.log &amp;6-2-2 step2_run_dim_feature_while.sh：加工user/sku/brand/user_sku/user_brand各维度特征，由于后续使用了滑窗集成，所以需要运行多份(part1-part12)。运行命令：nohup sh step2_run_dim_feature_while.sh &gt; tmp.log &amp;6-2-3 step3_run_merge_feature_while.sh：将各个维度的特征汇总为一张宽表，便于后续进行抽样及模型训练。由于后续使用了滑窗集成，所以需要运行多份(part1-part12)。运行命令：nohup sh step3_run_merge_feature_while.sh &gt; tmp.log &amp;step3_run_predict_feature.sh：预测指标加工运行命令：nohup sh step3_run_predict_feature.sh &gt; tmp.log &amp;6-2-4 step4_run_sample_feature_while.sh：对加工的特征宽表进行采样运行命令：nohup sh step4_run_sample_feature_while.sh &gt; tmp.log &amp;由于模型训练是在风控集市跑的spark任务，因此若要让脚本在其它集市可用，需要手动修改脚本及配置文件【影响步骤3.2.5，3.2.6，3.2.7】 6-2-5 step5_jdata_runXgboost_train.sh：利用spark-xgboost进行模型训练运行命令：nohup sh step5_jdata_runXgboost_train.sh &gt; tmp.log &amp; 6-2-6 step6_jdata_runXgboost_stacking.sh：将spark-xgboost训练好的6个子模型，进行stacking LR融合运行命令：nohup sh step6_jdata_runXgboost_stacking.sh &gt; tmp.log &amp; 6-2-7 step7_jdata_runXgboost_stacking_predict.sh：利用3.2.6训练好的融合模型，预测用户在4.16-4.20会购买的user_sku对运行命令：nohup sh step7_jdata_runXgboost_stacking_predict.sh &gt; tmp.log &amp;6-2-8 待3.2.7步骤运行完后，需要手动将生成txt格式结果从hdfs目录down下来，并按比赛要求的格式整理输出hadoop fs –get hdfs://ns2/user/mart_risk/sjmei/tests/xgboost/result_stacking_v3_20160406 总结第一次参加数据挖掘类比赛，作为一名新手，通过本次比赛，学习了数据挖掘的整个流程，同时也进一步熟悉了spark ml框架的使用。其次，更多的是在实践过程中体会到了自身的不足，要想打好比赛，必须源于对业务的深入理解及数据的细致分析，而这一点恰恰是做的最不好的。比赛中没有花很多时间对数据进行深入理解与细致分析。在特征处理，调参方面也做的很糙。 solo比赛很累，思维也很受限，只知道堆特征+xgboost+lr融合的方案，看到排行榜上其他同学的成绩都在噌噌地往上涨，而自己又不知道该如何优化涨分，成绩也一直停滞不前，以后要多向大牛学习以及多和别的同学一起交流学习。作为一名CS专业毕业的人，对于使用各种数据挖掘工具进行技术实现不是什么问题，但其实对于数据挖掘来说，更重要的还是分析建模能力，对业务的感知能力，自己这方面还很欠缺，今后需要多多加强。感谢公司举办的此次大赛，让我获益良多！]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Xgboost</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[xgboost4j编译安装笔记]]></title>
    <url>%2F2017%2F05%2F05%2Fxgboost-install%2F</url>
    <content type="text"><![CDATA[clone工程git clone --recursive https://github.com/dmlc/xgboost.git xgboost编译cd xgboost; make clean_all &amp;&amp; make -j4 xgboost4j编译cd jvm-packages;mvn clean &amp;&amp; mvn -Dmaven.test.skip=true -Dspark.version=2.1.0 package xgboost python编译cd python-package;python setup.py install Refrence1. https://xgboost.readthedocs.io/en/latest/build.html]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>xgboost4j</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark ML离线训练模型用于在线预测]]></title>
    <url>%2F2017%2F01%2F10%2Fspark-ml-realtime-predict%2F</url>
    <content type="text"><![CDATA[最近公司有需求需要将离线训练好的算法模型应用到线上去实时预测，在线预测不考虑feature加工的情况下，经调研，发现jpmml-sparkml+jpmml-evaluator的方式可以满足条件。不过使用时需要注意该框架是AGPL-3.0协议。 方案：spark ml + jpmml-sparkml + jpmml-evaluatorSpark离线训练Random Forest模型并保存为pmml格式：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266import java.io.FileOutputStreamimport javax.xml.transform.stream.StreamResultimport com.jd.risk.utils.HadoopFileUtilimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.Pathimport org.apache.spark.examples.ml.DecisionTreeExampleimport org.apache.spark.examples.mllib.AbstractParamsimport org.apache.spark.ml.classification.&#123;RandomForestClassificationModel, RandomForestClassifier&#125;import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.ml.feature.&#123;StringIndexer, VectorAssembler, VectorIndexer&#125;import org.apache.spark.ml.regression.&#123;RandomForestRegressionModel, RandomForestRegressor&#125;import org.apache.spark.ml.&#123;Pipeline, PipelineStage&#125;import org.apache.spark.sql.&#123;DataFrame, SparkSession&#125;import org.jpmml.model.&#123;JAXBUtil, MetroJAXBUtil&#125;import org.jpmml.sparkml.ConverterUtilimport scopt.OptionParserimport scala.collection.mutableimport scala.language.reflectiveCalls/** * Created by cdmeishangjian on 2017/01/19. */object RandomForestPMMLTask &#123; case class Params( input: String = null, modelDir: String = null, taskType:String = "train", testInput: String = "", dataFormat: String = "libsvm", algo: String = "classification", maxDepth: Int = 4, maxBins: Int = 32, minInstancesPerNode: Int = 1, minInfoGain: Double = 0.0, numTrees: Int = 5, featureSubsetStrategy: String = "auto", fracTest: Double = 0.2, cacheNodeIds: Boolean = false, checkpointDir: Option[String] = None, checkpointInterval: Int = 10) extends AbstractParams[Params] def main(args: Array[String]) &#123; val defaultParams = Params() val parser = new OptionParser[Params]("RandomForestExample") &#123; head("RandomForestExample: an example random forest app.") opt[String]("algo") .text(s"algorithm (classification, regression), default: $&#123;defaultParams.algo&#125;") .action((x, c) =&gt; c.copy(algo = x)) opt[String]("taskType") .text(s"modelType, default: $&#123;defaultParams.taskType&#125;") .action((x, c) =&gt; c.copy(taskType = x)) opt[Int]("maxDepth") .text(s"max depth of the tree, default: $&#123;defaultParams.maxDepth&#125;") .action((x, c) =&gt; c.copy(maxDepth = x)) opt[Int]("maxBins") .text(s"max number of bins, default: $&#123;defaultParams.maxBins&#125;") .action((x, c) =&gt; c.copy(maxBins = x)) opt[Int]("minInstancesPerNode") .text(s"min number of instances required at child nodes to create the parent split," + s" default: $&#123;defaultParams.minInstancesPerNode&#125;") .action((x, c) =&gt; c.copy(minInstancesPerNode = x)) opt[Double]("minInfoGain") .text(s"min info gain required to create a split, default: $&#123;defaultParams.minInfoGain&#125;") .action((x, c) =&gt; c.copy(minInfoGain = x)) opt[Int]("numTrees") .text(s"number of trees in ensemble, default: $&#123;defaultParams.numTrees&#125;") .action((x, c) =&gt; c.copy(numTrees = x)) opt[String]("featureSubsetStrategy") .text(s"number of features to use per node (supported:" + s" $&#123;RandomForestClassifier.supportedFeatureSubsetStrategies.mkString(",")&#125;)," + s" default: $&#123;defaultParams.numTrees&#125;") .action((x, c) =&gt; c.copy(featureSubsetStrategy = x)) opt[Double]("fracTest") .text(s"fraction of data to hold out for testing. If given option testInput, " + s"this option is ignored. default: $&#123;defaultParams.fracTest&#125;") .action((x, c) =&gt; c.copy(fracTest = x)) opt[Boolean]("cacheNodeIds") .text(s"whether to use node Id cache during training, " + s"default: $&#123;defaultParams.cacheNodeIds&#125;") .action((x, c) =&gt; c.copy(cacheNodeIds = x)) opt[String]("checkpointDir") .text(s"checkpoint directory where intermediate node Id caches will be stored, " + s"default: $&#123; defaultParams.checkpointDir match &#123; case Some(strVal) =&gt; strVal case None =&gt; "None" &#125; &#125;") .action((x, c) =&gt; c.copy(checkpointDir = Some(x))) opt[Int]("checkpointInterval") .text(s"how often to checkpoint the node Id cache, " + s"default: $&#123;defaultParams.checkpointInterval&#125;") .action((x, c) =&gt; c.copy(checkpointInterval = x)) opt[String]("testInput") .text(s"input path to test dataset. If given, option fracTest is ignored." + s" default: $&#123;defaultParams.testInput&#125;") .action((x, c) =&gt; c.copy(testInput = x)) opt[String]("dataFormat") .text("data format: libsvm (default), dense (deprecated in Spark v1.1)") .action((x, c) =&gt; c.copy(dataFormat = x)) arg[String]("&lt;input&gt;") .text("input path to labeled examples") .required() .action((x, c) =&gt; c.copy(input = x)) arg[String]("&lt;modelDir&gt;") .text("modelDir path to labeled examples") .required() .action((x, c) =&gt; c.copy(modelDir = x)) checkConfig &#123; params =&gt; if (params.fracTest &lt; 0 || params.fracTest &gt;= 1) &#123; failure(s"fracTest $&#123;params.fracTest&#125; value incorrect; should be in [0,1).") &#125; else &#123; success &#125; &#125; &#125; parser.parse(args, defaultParams) match &#123; case Some(params) =&gt; &#123; if(params.taskType.equalsIgnoreCase("train"))&#123; train(params) &#125; &#125; case _ =&gt; sys.exit(1) &#125; &#125; def train(params: Params): Unit = &#123; val spark = SparkSession .builder .master("local") .appName(s"RandomForestExample with $params") .getOrCreate() params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) val algo = params.algo.toLowerCase println(s"RandomForestExample with parameters:\n$params") // Load training and test data and cache it. val (training: DataFrame, test: DataFrame) = AlgoUtils.loadMaliceDataFrame(spark.sparkContext, params.input, params.fracTest) // Set up Pipeline. val stages = new mutable.ArrayBuffer[PipelineStage]() // (1) For classification, re-index classes. val labelColName = if (algo == "classification") "indexedLabel" else "label" if (algo == "classification") &#123; val labelIndexer = new StringIndexer() .setInputCol("label") .setOutputCol(labelColName) stages += labelIndexer &#125; val vectorAssember = new VectorAssembler() vectorAssember.setInputCols(Array("degree","tcNum","pageRank","commVertexNum","normQ","gtRate","eqRate","ltRate")) vectorAssember.setOutputCol("features") stages += vectorAssember // (2) Identify categorical features using VectorIndexer. // Features with more than maxCategories values will be treated as continuous. val featuresIndexer = new VectorIndexer() .setInputCol("features") .setOutputCol("indexedFeatures") .setMaxCategories(10) stages += featuresIndexer // (3) Learn Random Forest. val dt = algo match &#123; case "classification" =&gt; new RandomForestClassifier() .setFeaturesCol("features") .setLabelCol(labelColName) .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setFeatureSubsetStrategy(params.featureSubsetStrategy) .setNumTrees(params.numTrees) case "regression" =&gt; new RandomForestRegressor() .setFeaturesCol("features") .setLabelCol(labelColName) .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setFeatureSubsetStrategy(params.featureSubsetStrategy) .setNumTrees(params.numTrees) case _ =&gt; throw new IllegalArgumentException("Algo $&#123;params.algo&#125; not supported.") &#125; stages += dt val pipeline = new Pipeline().setStages(stages.toArray) // Fit the Pipeline. val startTime = System.nanoTime() val pipelineModel = pipeline.fit(training) val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") val rfModel = pipelineModel.stages.last.asInstanceOf[RandomForestClassificationModel] /** * write model pmml format to hdfs */ val modelPmmlPath = params.modelDir val pmml = ConverterUtil.toPMML(training.schema, pipelineModel); val conf = new Configuration(); HadoopFileUtil.deleteFile(modelPmmlPath) val path = new Path(modelPmmlPath); val fs = path.getFileSystem(conf); val out = fs.create(path); MetroJAXBUtil.marshalPMML(pmml, out); MetroJAXBUtil.marshalPMML(pmml, new FileOutputStream(modelPmmlPath)); JAXBUtil.marshalPMML(pmml, new StreamResult(System.out)); val predictions = pipelineModel.transform(training) // Get the trained Random Forest from the fitted PipelineModel. algo match &#123; case "classification" =&gt; if (rfModel.totalNumNodes &lt; 30) &#123; println(rfModel.toDebugString) // Print full model. &#125; else &#123; println(rfModel) // Print model summary. &#125; case "regression" =&gt; val rfrModel = pipelineModel.stages.last.asInstanceOf[RandomForestRegressionModel] if (rfrModel.totalNumNodes &lt; 30) &#123; println(rfrModel.toDebugString) // Print full model. &#125; else &#123; println(rfrModel) // Print model summary. &#125; case _ =&gt; throw new IllegalArgumentException("Algo $&#123;params.algo&#125; not supported.") &#125; // Evaluate model on training, test data. algo match &#123; case "classification" =&gt; println("Training data results:") DecisionTreeExample.evaluateClassificationModel(pipelineModel, training, labelColName) val evaluator = new MulticlassClassificationEvaluator() .setLabelCol("indexedLabel") .setPredictionCol("prediction") .setMetricName("accuracy") val accuracy = evaluator.evaluate(predictions) println("Test Error = " + (1.0 - accuracy)) case "regression" =&gt; println("Training data results:") DecisionTreeExample.evaluateRegressionModel(pipelineModel, training, labelColName) case _ =&gt; throw new IllegalArgumentException("Algo $&#123;params.algo&#125; not supported.") &#125; predictions.printSchema() predictions.select("label","prediction","probability").show(10) spark.stop() &#125;&#125; jpmml-evaluator实现在线实时预测：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111/** * Created by cdmeishangjian on 2017/1/19. */public class PrdictScore &#123; public static void main(String[] args) throws Exception &#123; PMML pmml = readPMML(new File("data/pmmlmodel/rf.pmml")); ModelEvaluatorFactory modelEvaluatorFactory = ModelEvaluatorFactory.newInstance();// System.out.println(pmml.getModels().get(0)); Evaluator evaluator = modelEvaluatorFactory.newModelEvaluator(pmml);// ModelEvaluator evaluator = new MiningModelEvaluator(pmml); evaluator.verify(); List&lt;InputField&gt; inputFields = evaluator.getInputFields(); InputStream is = new FileInputStream(new File("data/train.txt")); BufferedReader br = new BufferedReader(new InputStreamReader(is)); String line; int diffDelta = 0; int sameDelta = 0; while((line = br.readLine()) != null) &#123; String[] splits = line.split("\t",-1); double targetMs = transToDouble(splits[14]); double risk_value = transToDouble(splits[2]); double label = 0.0; if(targetMs==1.0 &amp;&amp; risk_value &gt;5.0d)&#123; label = 1.0; &#125; LinkedHashMap&lt;FieldName, FieldValue&gt; arguments = readArgumentsFromLine(splits, inputFields); Map&lt;FieldName, ?&gt; results = evaluator.evaluate(arguments); List&lt;TargetField&gt; targetFields = evaluator.getTargetFields(); for(TargetField targetField : targetFields)&#123; FieldName targetFieldName = targetField.getName(); Object targetFieldValue = results.get(targetFieldName); ProbabilityDistribution nodeMap = (ProbabilityDistribution)targetFieldValue; Object result = nodeMap.getResult(); if(label == Double.valueOf(result.toString()))&#123; sameDelta +=1; &#125;else&#123; diffDelta +=1; &#125; &#125; &#125; System.out.println("acc count:"+sameDelta); System.out.println("error count:"+diffDelta); System.out.println("acc rate:"+(sameDelta*1.0d/(sameDelta+diffDelta))); &#125; /** * 从文件中读取pmml模型文件 * @param file * @return * @throws Exception */ public static PMML readPMML(File file) throws Exception &#123; InputStream is = new FileInputStream(file); return PMMLUtil.unmarshal(is); &#125; /** * 构造模型输入特征字段 * @param splits * @param inputFields * @return */ public static LinkedHashMap&lt;FieldName, FieldValue&gt; readArgumentsFromLine(String[] splits, List&lt;InputField&gt; inputFields) &#123; List&lt;Double&gt; lists = new ArrayList&lt;Double&gt;(); lists.add(Double.valueOf(splits[3])); lists.add(Double.valueOf(splits[4])); lists.add(Double.valueOf(splits[5])); lists.add(Double.valueOf(splits[7])); lists.add(Double.valueOf(splits[8])); lists.add(Double.valueOf(splits[9])); lists.add(Double.valueOf(splits[10])); lists.add(Double.valueOf(splits[11])); LinkedHashMap&lt;FieldName, FieldValue&gt; arguments = new LinkedHashMap&lt;FieldName, FieldValue&gt;(); int i = 0; for(InputField inputField : inputFields)&#123; FieldName inputFieldName = inputField.getName(); // The raw (ie. user-supplied) value could be any Java primitive value Object rawValue = lists.get(i); // The raw value is passed through: 1) outlier treatment, 2) missing value treatment, 3) invalid value treatment and 4) type conversion FieldValue inputFieldValue = inputField.prepare(rawValue); arguments.put(inputFieldName, inputFieldValue); i+=1; &#125; return arguments; &#125; public static Double transToDouble(String label) &#123; try &#123; return Double.valueOf(label); &#125;catch (Exception e)&#123; return Double.valueOf(0); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkML GBDT&RF算法使用示例]]></title>
    <url>%2F2017%2F01%2F10%2Fspark-gbdt-rf%2F</url>
    <content type="text"><![CDATA[GBDT与RF作为机器学习中最常用的两个集成学习算法，Spark中也有相应的实现。下面是基于Spark 2.1.0 GBDT与RF算法的训练与预测(train/predict)接口实现。功能： - train(训练)/train_cv(训练+网格搜索参数优化+交叉验证)/predict(预测)接口 Random Forest算法train/train_cv/predict实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302import com.sjmei.jdata.utils.&#123;AlgoUtils, DataLoadUtils&#125;import org.apache.spark.examples.mllib.AbstractParamsimport org.apache.spark.ml.classification.&#123;RandomForestClassificationModel, RandomForestClassifier&#125;import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.ml.tuning.&#123;CrossValidator, CrossValidatorModel, ParamGridBuilder&#125;import org.apache.spark.ml.&#123;Pipeline, PipelineStage&#125;import org.apache.spark.sql.DataFrameimport scopt.OptionParserimport scala.collection.mutableimport scala.language.reflectiveCalls/** * Created by cdmeishangjian on 2016/10/20. * */object RandomForestTaskTest &#123; case class Params( input: String = null, modelDir: String = null, output: String = null, taskType: String = null, dataFormat: String = "libsvm", resultType: String = "rf_predict_eval", maxDepth: Int = 10, maxBins: Int = 50, minInstancesPerNode: Int = 1, minInfoGain: Double = 0.0, numTrees: Int = 100, featureSubsetStrategy: String = "auto", isCvModel: Boolean = false, fracTest: Double = 0.1, cacheNodeIds: Boolean = false, checkpointDir: Option[String] = None, checkpointInterval: Int = 10) extends AbstractParams[Params] def main(args: Array[String]) &#123; val defaultParams = Params() val parser = new OptionParser[Params]("RandomForestExample") &#123; head("RandomForestExample: an example random forest app.") opt[String]("resultType") .text(s"algorithm (classification, regression), default: $&#123;defaultParams.resultType&#125;") .action((x, c) =&gt; c.copy(resultType = x)) opt[Int]("maxDepth") .text(s"max depth of the tree, default: $&#123;defaultParams.maxDepth&#125;") .action((x, c) =&gt; c.copy(maxDepth = x)) opt[Int]("maxBins") .text(s"max number of bins, default: $&#123;defaultParams.maxBins&#125;") .action((x, c) =&gt; c.copy(maxBins = x)) opt[Int]("minInstancesPerNode") .text(s"min number of instances required at child nodes to create the parent split," + s" default: $&#123;defaultParams.minInstancesPerNode&#125;") .action((x, c) =&gt; c.copy(minInstancesPerNode = x)) opt[Double]("minInfoGain") .text(s"min info gain required to create a split, default: $&#123;defaultParams.minInfoGain&#125;") .action((x, c) =&gt; c.copy(minInfoGain = x)) opt[Int]("numTrees") .text(s"number of trees in ensemble, default: $&#123;defaultParams.numTrees&#125;") .action((x, c) =&gt; c.copy(numTrees = x)) opt[String]("featureSubsetStrategy") .text(s"number of features to use per node (supported:" + s" $&#123;RandomForestClassifier.supportedFeatureSubsetStrategies.mkString(",")&#125;)," + s" default: $&#123;defaultParams.numTrees&#125;") .action((x, c) =&gt; c.copy(featureSubsetStrategy = x)) opt[Double]("fracTest") .text(s"fraction of data to hold out for testing. If given option testInput, " + s"this option is ignored. default: $&#123;defaultParams.fracTest&#125;") .action((x, c) =&gt; c.copy(fracTest = x)) opt[Boolean]("isCvModel") .text("is cvmodel flag: false (default)") .action((x, c) =&gt; c.copy(isCvModel = x)) opt[Boolean]("cacheNodeIds") .text(s"whether to use node Id cache during training, " + s"default: $&#123;defaultParams.cacheNodeIds&#125;") .action((x, c) =&gt; c.copy(cacheNodeIds = x)) opt[String]("checkpointDir") .text(s"checkpoint directory where intermediate node Id caches will be stored, " + s"default: $&#123; defaultParams.checkpointDir match &#123; case Some(strVal) =&gt; strVal case None =&gt; "None" &#125; &#125;") .action((x, c) =&gt; c.copy(checkpointDir = Some(x))) opt[Int]("checkpointInterval") .text(s"how often to checkpoint the node Id cache, " + s"default: $&#123;defaultParams.checkpointInterval&#125;") .action((x, c) =&gt; c.copy(checkpointInterval = x)) opt[String]("dataFormat") .text("data format: libsvm (default), dense (deprecated in Spark v1.1)") .action((x, c) =&gt; c.copy(dataFormat = x)) arg[String]("&lt;input&gt;") .text("input path to labeled examples") .required() .action((x, c) =&gt; c.copy(input = x)) arg[String]("&lt;modelDir&gt;") .text("modelDir path to labeled examples") .required() .action((x, c) =&gt; c.copy(modelDir = x)) arg[String]("&lt;output&gt;") .text("output path to labeled examples") .required() .action((x, c) =&gt; c.copy(output = x)) arg[String]("&lt;taskType&gt;") .text("train or predict the rf model") .required() .action((x, c) =&gt; c.copy(taskType = x)) checkConfig &#123; params =&gt; if (params.fracTest &lt; 0 || params.fracTest &gt;= 1) &#123; failure(s"fracTest $&#123;params.fracTest&#125; value incorrect; should be in [0,1).") &#125; else &#123; success &#125; &#125; &#125; parser.parse(args, defaultParams) match &#123; case Some(params) =&gt; &#123; params.taskType.trim.toLowerCase match &#123; case "train" =&gt; train(params) case "train_cv" =&gt; train_cv(params) case "predict" =&gt; predict(params) case _ =&gt; println("XGBoost method error...") &#125; &#125; case _ =&gt; sys.exit(1) &#125; &#125; def train(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"RandomForestExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) val algo = params.resultType.toLowerCase println(s"RandomForestExample with parameters:\n$params") // Load training and test data and cache it. val (training: DataFrame, test: DataFrame) = DataLoadUtils.loadTrainData(spark, params.input, params.fracTest) // Set up Pipeline. val stages = new mutable.ArrayBuffer[PipelineStage]() val dt = new RandomForestClassifier() .setFeaturesCol("features") .setLabelCol("label") .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setFeatureSubsetStrategy(params.featureSubsetStrategy) .setNumTrees(params.numTrees) stages += dt val pipeline = new Pipeline().setStages(stages.toArray) // Fit the Pipeline. val startTime = System.nanoTime() val pipelineModel = pipeline.fit(training) val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") val rfModel = pipelineModel.stages.last.asInstanceOf[RandomForestClassificationModel] rfModel.write.overwrite.save(params.modelDir) val predictions = pipelineModel.transform(training) val df_test_pred = pipelineModel.transform(test) // Get the trained Random Forest from the fitted PipelineModel. if (rfModel.totalNumNodes &lt; 30) &#123; println(rfModel.toDebugString) // Print full model. &#125; else &#123; println(rfModel) // Print model summary. &#125; // Evaluate model on training, test data. println("Training &amp; Testing data evaluate results:") val evaluator = new MulticlassClassificationEvaluator() .setLabelCol("label") .setPredictionCol("prediction") val train_accuracy = evaluator.evaluate(predictions) val test_accuracy = evaluator.evaluate(df_test_pred) println(s"Train Accuracy = $train_accuracy, Test Accuracy = $test_accuracy") predictions.printSchema() spark.stop() &#125; def train_cv(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"RandomForestExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) val algo = params.resultType.toLowerCase println(s"RandomForestExample with parameters:\n$params") // Load training and test data and cache it. val (training: DataFrame, test: DataFrame) = DataLoadUtils.loadTrainData(spark, params.input, params.fracTest) // Set up Pipeline. val stages = new mutable.ArrayBuffer[PipelineStage]() val dt = new RandomForestClassifier() .setFeaturesCol("features") .setLabelCol("label") .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setFeatureSubsetStrategy(params.featureSubsetStrategy) .setNumTrees(params.numTrees) stages += dt val pipeline = new Pipeline().setStages(stages.toArray) // Fit the Pipeline. val startTime = System.nanoTime() // We use a ParamGridBuilder to construct a grid of parameters to search over. val paramGrid = new ParamGridBuilder() .addGrid(dt.maxDepth, Array(8, 10)) .addGrid(dt.numTrees, Array(50, 100, 200)) .build() // We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance. // This will allow us to jointly choose parameters for all Pipeline stages. // A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator. // Note that the evaluator here is a BinaryClassificationEvaluator and its default metric // is areaUnderROC. val cv = new CrossValidator() .setEstimator(pipeline) .setEvaluator(new MulticlassClassificationEvaluator) .setEstimatorParamMaps(paramGrid) .setNumFolds(5) // Use 3+ in practice // Run cross-validation, and choose the best set of parameters. val cvModel = cv.fit(training) cvModel.write.overwrite.save(params.modelDir) // Make predictions on test documents. cvModel uses the best model found (lrModel). val train_predict = cvModel.transform(training).select("label","prediction","probability") val test_predict = cvModel.transform(test).select("label","prediction","probability") val evaluator = new MulticlassClassificationEvaluator() .setLabelCol("label") .setPredictionCol("prediction") val train_accuracy = evaluator.evaluate(train_predict) val test_accuracy = evaluator.evaluate(test_predict) println(s"Train Accuracy = $train_accuracy, Test Accuracy = $test_accuracy") // $example off$ val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") train_predict.printSchema() train_predict.select("label","prediction","probability").show(10) spark.stop() &#125; def predict(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"RandomForestExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) println(s"RandomForestExample with parameters:\n$params") // Load training and test data and cache it. val datasets = DataLoadUtils.loadPredictDataOrc(spark, params.input) // Fit the Pipeline. val startTime = System.nanoTime() var predictions: DataFrame = null if(params.isCvModel)&#123; val predModel = CrossValidatorModel.load(params.modelDir) predictions = predModel.transform(datasets) &#125;else&#123; val predModel = RandomForestClassificationModel.load(params.modelDir) predictions = predModel.transform(datasets) &#125; val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") AlgoUtils.saveRFPredictResult(spark, predictions, params) datasets.unpersist(blocking = false) predictions.unpersist(blocking = false) spark.stop() &#125;&#125; GBDT算法train/train_cv/predict实现:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296import com.sjmei.jdata.utils.&#123;AlgoUtils, DataLoadUtils&#125;import org.apache.spark.examples.mllib.AbstractParamsimport org.apache.spark.ml.classification.&#123;GBTClassificationModel, GBTClassifier&#125;import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluatorimport org.apache.spark.ml.tuning.&#123;CrossValidator, CrossValidatorModel, ParamGridBuilder&#125;import org.apache.spark.ml.&#123;Pipeline, PipelineStage&#125;import org.apache.spark.sql.DataFrameimport org.apache.spark.utils.Loggingimport scopt.OptionParserimport scala.collection.mutableimport scala.language.reflectiveCalls/** * * Created by cdmeishangjian on 2016/10/26. * */object GBTTaskTest extends Logging &#123; case class Params( input: String = null, modelDir: String = null, output: String = null, taskType:String = null, dataFormat: String = "libsvm", algo: String = "classification", maxDepth: Int = 10, maxBins: Int = 50, minInstancesPerNode: Int = 1, minInfoGain: Double = 0.0, maxIter: Int = 100, fracTest: Double = 0.1, isCvModel: Boolean = false, cacheNodeIds: Boolean = false, checkpointDir: Option[String] = None, checkpointInterval: Int = 10) extends AbstractParams[Params] def main(args: Array[String]) &#123; val defaultParams = Params() val parser = new OptionParser[Params]("GBTExample") &#123; head("GBTExample: an example Gradient-Boosted Trees app.") opt[String]("algo") .text(s"algorithm (classification, regression), default: $&#123;defaultParams.algo&#125;") .action((x, c) =&gt; c.copy(algo = x)) opt[Int]("maxDepth") .text(s"max depth of the tree, default: $&#123;defaultParams.maxDepth&#125;") .action((x, c) =&gt; c.copy(maxDepth = x)) opt[Int]("maxBins") .text(s"max number of bins, default: $&#123;defaultParams.maxBins&#125;") .action((x, c) =&gt; c.copy(maxBins = x)) opt[Int]("minInstancesPerNode") .text(s"min number of instances required at child nodes to create the parent split," + s" default: $&#123;defaultParams.minInstancesPerNode&#125;") .action((x, c) =&gt; c.copy(minInstancesPerNode = x)) opt[Double]("minInfoGain") .text(s"min info gain required to create a split, default: $&#123;defaultParams.minInfoGain&#125;") .action((x, c) =&gt; c.copy(minInfoGain = x)) opt[Int]("maxIter") .text(s"number of trees in ensemble, default: $&#123;defaultParams.maxIter&#125;") .action((x, c) =&gt; c.copy(maxIter = x)) opt[Double]("fracTest") .text(s"fraction of data to hold out for testing. If given option testInput, " + s"this option is ignored. default: $&#123;defaultParams.fracTest&#125;") .action((x, c) =&gt; c.copy(fracTest = x)) opt[Boolean]("isCvModel") .text("is cvmodel flag: false (default)") .action((x, c) =&gt; c.copy(isCvModel = x)) opt[Boolean]("cacheNodeIds") .text(s"whether to use node Id cache during training, " + s"default: $&#123;defaultParams.cacheNodeIds&#125;") .action((x, c) =&gt; c.copy(cacheNodeIds = x)) opt[String]("checkpointDir") .text(s"checkpoint directory where intermediate node Id caches will be stored, " + s"default: $&#123; defaultParams.checkpointDir match &#123; case Some(strVal) =&gt; strVal case None =&gt; "None" &#125; &#125;") .action((x, c) =&gt; c.copy(checkpointDir = Some(x))) opt[Int]("checkpointInterval") .text(s"how often to checkpoint the node Id cache, " + s"default: $&#123;defaultParams.checkpointInterval&#125;") .action((x, c) =&gt; c.copy(checkpointInterval = x)) opt[String]("dataFormat") .text("data format: libsvm (default), dense (deprecated in Spark v1.1)") .action((x, c) =&gt; c.copy(dataFormat = x)) arg[String]("&lt;input&gt;") .text("input path to labeled examples") .required() .action((x, c) =&gt; c.copy(input = x)) arg[String]("&lt;modelDir&gt;") .text("modelDir path to labeled examples") .required() .action((x, c) =&gt; c.copy(modelDir = x)) arg[String]("&lt;output&gt;") .text("output path to labeled examples") .required() .action((x, c) =&gt; c.copy(output = x)) arg[String]("&lt;taskType&gt;") .text("train or predict the rf model") .required() .action((x, c) =&gt; c.copy(taskType = x)) checkConfig &#123; params =&gt; if (params.fracTest &lt; 0 || params.fracTest &gt;= 1) &#123; failure(s"fracTest $&#123;params.fracTest&#125; value incorrect; should be in [0,1).") &#125; else &#123; success &#125; &#125; &#125; parser.parse(args, defaultParams) match &#123; case Some(params) =&gt; &#123; params.taskType.trim.toLowerCase match &#123; case "train" =&gt; train(params) case "train_cv" =&gt; train_cv(params) case "predict" =&gt; predict(params) case _ =&gt; println("XGBoost method error...") &#125; &#125; case _ =&gt; sys.exit(1) &#125; &#125; def train(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"GBTExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) val algo = params.algo.toLowerCase println(s"GBTExample with parameters:\n$params") // Load training and test data and cache it. val (training: DataFrame, test: DataFrame) = DataLoadUtils.loadTrainData(spark, params.input, params.fracTest) // Set up Pipeline val stages = new mutable.ArrayBuffer[PipelineStage]() // Learn GBT. val dt = new GBTClassifier() .setFeaturesCol("features") .setLabelCol("label") .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setMaxIter(params.maxIter) stages += dt val pipeline = new Pipeline().setStages(stages.toArray) // Fit the Pipeline. val startTime = System.nanoTime() val pipelineModel = pipeline.fit(training) val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") val gbtModel = pipelineModel.stages.last.asInstanceOf[GBTClassificationModel] gbtModel.write.overwrite.save(params.modelDir) val predictions = pipelineModel.transform(training) val df_test_pred = pipelineModel.transform(test) // Get the trained GBT from the fitted PipelineModel. if (gbtModel.totalNumNodes &lt; 30) &#123; println(gbtModel.toDebugString) // Print full model. &#125; else &#123; println(gbtModel) // Print model summary. &#125; // Evaluate model on training, test data. println("Training &amp; Testing data evaluate results:") val evaluator = new MulticlassClassificationEvaluator() .setLabelCol("label") .setPredictionCol("prediction") .setMetricName("accuracy") val train_accuracy = evaluator.evaluate(predictions) val test_accuracy = evaluator.evaluate(df_test_pred) println(s"Train Accuracy = $train_accuracy, Test Accuracy = $test_accuracy") AlgoUtils.saveNormProbResult(spark, predictions, params.output) predictions.printSchema() predictions.select("prediction","rawPrediction","probability", "label", "features").show(5) spark.stop() &#125; def train_cv(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"GBTExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) val algo = params.algo.toLowerCase println(s"GBTExample with parameters:\n$params") // Load training and test data and cache it. val (training: DataFrame, test: DataFrame) = DataLoadUtils.loadTrainData(spark, params.input, params.fracTest) // Set up Pipeline. val stages = new mutable.ArrayBuffer[PipelineStage]() // Learn GBT. val dt = new GBTClassifier() .setFeaturesCol("features") .setLabelCol("label") .setMaxDepth(params.maxDepth) .setMaxBins(params.maxBins) .setMinInstancesPerNode(params.minInstancesPerNode) .setMinInfoGain(params.minInfoGain) .setCacheNodeIds(params.cacheNodeIds) .setCheckpointInterval(params.checkpointInterval) .setMaxIter(params.maxIter) stages += dt val pipeline = new Pipeline().setStages(stages.toArray) // Fit the Pipeline. val startTime = System.nanoTime() // We use a ParamGridBuilder to construct a grid of parameters to search over. val paramGrid = new ParamGridBuilder() .addGrid(dt.maxDepth, Array(8, 10)) .addGrid(dt.maxIter, Array(50, 100, 200)) .build() // We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance. // This will allow us to jointly choose parameters for all Pipeline stages. // A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator. // Note that the evaluator here is a BinaryClassificationEvaluator and its default metric // is areaUnderROC. val cv = new CrossValidator() .setEstimator(pipeline) .setEvaluator(new MulticlassClassificationEvaluator) .setEstimatorParamMaps(paramGrid) .setNumFolds(5) // Use 3+ in practice // Run cross-validation, and choose the best set of parameters. val cvModel = cv.fit(training) cvModel.write.overwrite.save(params.modelDir) // Make predictions on test documents. cvModel uses the best model found (lrModel). val train_predict = cvModel.transform(training).select("label","prediction","probability") val test_predict = cvModel.transform(test).select("label","prediction","probability") val evaluator = new MulticlassClassificationEvaluator() .setLabelCol("label") .setPredictionCol("prediction") val train_accuracy = evaluator.evaluate(train_predict) val test_accuracy = evaluator.evaluate(test_predict) println(s"Train Accuracy = $train_accuracy, Test Accuracy = $test_accuracy") // $example off$ val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") train_predict.printSchema() train_predict.select("label","prediction","probability").show(10) spark.stop() &#125; def predict(params: Params): Unit = &#123; val spark = AlgoUtils.getSparkSession(s"GBTExample with $params") params.checkpointDir.foreach(spark.sparkContext.setCheckpointDir) println(s"GBTExample with parameters:\n$params") // Load training and test data and cache it. val datasets = DataLoadUtils.loadPredictDataOrc(spark,params.input) // Fit the Pipeline. val startTime = System.nanoTime() var results: DataFrame = null if(params.isCvModel)&#123; val predModel = CrossValidatorModel.load(params.modelDir) results = predModel.transform(datasets) &#125;else&#123; val predModel = GBTClassificationModel.load(params.modelDir) results = predModel.transform(datasets) &#125; val elapsedTime = (System.nanoTime() - startTime) / 1e9 println(s"Training time: $elapsedTime seconds") AlgoUtils.saveNormProbResult(spark, results, params.output) datasets.unpersist(blocking = false) results.unpersist(blocking = false) spark.stop() &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Random Forest</tag>
        <tag>GBDT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda/tensorflow/keras安装笔记]]></title>
    <url>%2F2017%2F01%2F05%2Fcuda-install%2F</url>
    <content type="text"><![CDATA[sudo yum install kernel-devel-$(uname -r)、kernel-headers-$(uname -r) 手动下载，rpm -ivh安装 禁用nouveau driver，创建 /etc/modprobe.d/blacklist-nouveau.conf: 12blacklist nouveauoptions nouveau modeset=0 Regenerate the kernel initramfs and 重启机器: sudo dracut --force sh cuda_8.0.44_linux.run(全部默认选项) &amp; chmod 755 -R /usr/local/cuda-8.0 cudnn安装： 12345678tar -xzvf cudnn-8.0-linux-x64-v5.1.tgzcd cudnn-8.0-linux-x64-v5.1sudo cp lib64/* /usr/local/cuda/lib64/sudo cp include/* /usr/local/cuda/include/复制后软链接会失效，需要重新建立软链接：ln -s libcudnn.so.6.0.21 libcudnn.so.6ln -s libcudnn.so.6 libcudnn.so 设置环境变量: 12export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64export PATH="/usr/local/cuda-8.0/bin:$PATH" pip install tensorflow/theano/keras(若无法下载包，手动下载再安装) 1pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.2.0-cp35-cp35m-manylinux1_x86_64.whl]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>tensorflow</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客迁移笔记]]></title>
    <url>%2F2016%2F12%2F20%2Fhexo-transfer%2F</url>
    <content type="text"><![CDATA[Hexo博客迁移笔记发现pelican搭建的github博客，访问特别慢，而且模板样式不是很好看;网上调研了下，觉得hexo的模板特别简洁，访问速度也还可以，于是将原来基于pelican的博客引擎倒腾到hexo上来。 step 1: hexo install12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install hexo-cli -g step2: hexo init12345678910111213hexo init hexo-blog #执行init命令初始化到你指定的hexo目录cd hexo-blognpm install #install before start blogginghexo generate #自动根据当前目录下文件,生成静态网页hexo server #运行本地服务QA:1. localhost:4000无法访问你的电脑端口被占用了。hexo默认的端口是4000，如果你的电脑安装了福昕阅读器，就是他，没错，坑爹吧！！！！启动hexo s 的时候，用这个命令，换一个端口。`hexo s -p 5000`2. FATAL can not read a block mapping entry; a multiline key may not be an implicit key at line 13, column 1:检查_config.yml内容，特别注意`:`后面需要有一个空格 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150&lt;li&gt; &lt;a title="把这个链接拖到你的Chrome收藏夹工具栏中" href='javascript:(function() &#123; function c() &#123; var e = document.createElement("link"); e.setAttribute("type", "text/css"); e.setAttribute("rel", "stylesheet"); e.setAttribute("href", f); e.setAttribute("class", l); document.body.appendChild(e) &#125; function h() &#123; var e = document.getElementsByClassName(l); for (var t = 0; t &lt; e.length; t++) &#123; document.body.removeChild(e[t]) &#125; &#125; function p() &#123; var e = document.createElement("div"); e.setAttribute("class", a); document.body.appendChild(e); setTimeout(function() &#123; document.body.removeChild(e) &#125;, 100) &#125; function d(e) &#123; return &#123; height : e.offsetHeight, width : e.offsetWidth &#125; &#125; function v(i) &#123; var s = d(i); return s.height &gt; e &amp;&amp; s.height &lt; n &amp;&amp; s.width &gt; t &amp;&amp; s.width &lt; r &#125; function m(e) &#123; var t = e; var n = 0; while (!!t) &#123; n += t.offsetTop; t = t.offsetParent &#125; return n &#125; function g() &#123; var e = document.documentElement; if (!!window.innerWidth) &#123; return window.innerHeight &#125; else if (e &amp;&amp; !isNaN(e.clientHeight)) &#123; return e.clientHeight &#125; return 0 &#125; function y() &#123; if (window.pageYOffset) &#123; return window.pageYOffset &#125; return Math.max(document.documentElement.scrollTop, document.body.scrollTop) &#125; function E(e) &#123; var t = m(e); return t &gt;= w &amp;&amp; t &lt;= b + w &#125; function S() &#123; var e = document.createElement("audio"); e.setAttribute("class", l); e.src = i; e.loop = false; e.addEventListener("canplay", function() &#123; setTimeout(function() &#123; x(k) &#125;, 500); setTimeout(function() &#123; N(); p(); for (var e = 0; e &lt; O.length; e++) &#123; T(O[e]) &#125; &#125;, 15500) &#125;, true); e.addEventListener("ended", function() &#123; N(); h() &#125;, true); e.innerHTML = " &lt;p&gt;If you are reading this, it is because your browser does not support the audio element. We recommend that you get a new browser.&lt;/p&gt; &lt;p&gt;"; document.body.appendChild(e); e.play() &#125; function x(e) &#123; e.className += " " + s + " " + o &#125; function T(e) &#123; e.className += " " + s + " " + u[Math.floor(Math.random() * u.length)] &#125; function N() &#123; var e = document.getElementsByClassName(s); var t = new RegExp("\\b" + s + "\\b"); for (var n = 0; n &lt; e.length; ) &#123; e[n].className = e[n].className.replace(t, "") &#125; &#125; var e = 30; var t = 30; var n = 350; var r = 350; var i = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake.mp3"; var s = "mw-harlem_shake_me"; var o = "im_first"; var u = ["im_drunk", "im_baked", "im_trippin", "im_blown"]; var a = "mw-strobe_light"; var f = "//s3.amazonaws.com/moovweb-marketing/playground/harlem-shake-style.css"; var l = "mw_added_css"; var b = g(); var w = y(); var C = document.getElementsByTagName("*"); var k = null; for (var L = 0; L &lt; C.length; L++) &#123; var A = C[L]; if (v(A)) &#123; if (E(A)) &#123; k = A; break &#125; &#125; &#125; if (A === null) &#123; console.warn("Could not find a node of the right size. Please try a different page."); return &#125; c(); S(); var O = []; for (var L = 0; L &lt; C.length; L++) &#123; var A = C[L]; if (v(A)) &#123; O.push(A) &#125; &#125;&#125;)() '&gt;High一下&lt;/a&gt; &lt;/li&gt; step3:blog config123451. 修改or添加菜单项名：修改对应的翻译文件 languages/***.yml，在 menu 字段下添加一项 2. 设定菜单项图标，对应的字段是menu_icons，此设定格式是 item name: icon name(参考：Font Awesome 图标名字) 3. auto_excerpt:enable: truelength: 150 Reference: Hexo安装和配置 NexT使用文档 Font Awesome next-wiki fontawesome-cheatsheet 设置博客播放音乐]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB API学习笔记]]></title>
    <url>%2F2016%2F11%2F04%2Forientdb-learning-notes%2F</url>
    <content type="text"><![CDATA[OrientDB’s APIs: Document, Object, and Graph Records (or documents/vertices)attributes(or “fields” and “properties”) In OrientDB each record has its own self-assigned unique ID within the database called Record ID or RID. It is composed of two parts:#&lt;cluster-id&gt;:&lt;cluster-position&gt;cluster-id： is the id of the cluster. Each database can have a maximum of 32,767 clusters (2^15-1)cluster-position： is the position of the record inside the cluster. Each cluster can handle up to 9,223,372,036,854,780,000 (2^63) records, namely 9,223,372 Trillion of records! A RID (Record ID) is the physical position of the record inside the database. OrientDB has the concept of records as an element of storage. There are different types of records, document is one type of records. A document is composed of attributes(or “fields” and “properties”) and can belong to one class. Classes are also used in OrientDB as a type of data model according to certain rules. A cluster is a place where a group of records are stored. Perhaps the best equivalent in the relational world would be a Table. By default, OrientDB will create one cluster per class. All the records of a class are stored in the same cluster which has the same name as the class. You can create up to 32,767 (2^15-1) clusters in a database. The benefits of using different physical places to store records are: faster queries against clusters because only a sub-set of all the class’s clusters must be searchedgood partitioning allows you to reduce/remove the use of indexesparallel queries if on multiple diskssharding large data sets across multiple disks or server instancesThere are two types of clusters:Physical Cluster (known as local) which is persistent because it writes directly to the file systemMemory Cluster where everything is volatile and will be lost on termination of the process or server if the database is remote The most important feature of a graph database is the management of relationships With OrientDB, speed of traversal is not affected by the database size.It is always constant regardless if it has one record or 100 billion records. This is critical in the age of Big Data. OrientDB comes with a generic Vertex persistent class called “V” (OGraphVertex in previous releases) and “E” (OGraphEdge in the past) for Edge. lightweight edges: they don’t have own identities as record, but are physically stored as links inside vertices. OrientDB automatically uses Lightweight edges only when edges have no properties, otherwise regular edges are used.This is to improve performance and reduce the space on disk. But as a consequence, since lightweight edges don’t exist as separate records in the database Class –&gt;table(classes can be schema-full, schema-less, or mixed.)Property –&gt; columnRecord –&gt; rowCluster(store the data of a class in different physical locations) Console命令 连接数据库：connect remote:172.19.163.83/jrdm orientdb orientdb http://orientdb.com/docs/2.0/orientdb.wiki/Tutorial-Relationships.htmlhttp://orientdb.com/spark-orientdb/]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB分布式安装文档]]></title>
    <url>%2F2016%2F11%2F02%2Forientdb-distribute-install%2F</url>
    <content type="text"><![CDATA[step 1:在虚拟机centos上新建用户jdorientdb,下载orientdb包解压到 /software/orientdb目录step 2:设置ORIENTDB_HOME环境变量12345vim ~/.bash_rcexport ORIENTDB_HOME="your soft directory"source ~/.bash_rc step 3:修改config/orientdb-server-config.xml，设置用户登录密码123&lt;users&gt; &lt;user resources="*" password="jdorientdb" name="jdorientdb"/&gt;&lt;/users&gt; step 4:修改bin/orientdb.shorientdb启动默认需要root权限，修改配置去除root权限限制1234567891011# You have to SET the OrientDB installation directory hereORIENTDB_DIR="$ORIENTDB_HOME"ORIENTDB_USER="jdorientdb"start方法中注释root权限执行：#su $ORIENTDB_USER -c "cd \"$ORIENTDB_DIR/bin\"; /usr/bin/nohup ./server.sh 1&gt;$LOG_DIR/orientdb.log 2&gt;$LOG_DIR/orientdb.err &amp;stop方法中注释root权限执行：#su $ORIENTDB_USER -c "cd \"$ORIENTDB_DIR/bin\";/usr/bin/nohup ./shutdown.sh 1&gt;&gt;$LOG_DIR/orientdb.log 2&gt;&gt;$LOG_DIR/orientdb.err &amp; step 5:配置集群中各节点发现服务，修改configh/azecast.xml1234567891011121314151617181920&lt;group&gt; &lt;name&gt;jdorientdb&lt;/name&gt; &lt;password&gt;jdorientdb&lt;/password&gt;&lt;/group&gt;&lt;network&gt; &lt;port auto-increment="true"&gt;2434&lt;/port&gt; &lt;join&gt; &lt;multicast enabled="false"&gt; &lt;multicast-group&gt;235.1.1.1&lt;/multicast-group&gt; &lt;multicast-port&gt;2434&lt;/multicast-port&gt; &lt;/multicast&gt; &lt;tcp-ip enabled="true"&gt;#配置待优化 &lt;member&gt;jdorientdb1:2434&lt;/member&gt; &lt;member&gt;jdorientdb2:2434&lt;/member&gt; &lt;member&gt;jdorientdb3:2434&lt;/member&gt; &lt;member&gt;jdorientdb4:2434&lt;/member&gt; &lt;/tcp-ip&gt; &lt;/join&gt;&lt;/network&gt; step6:修改config/default-distributed-db-config.json1234567891011121314151617181920&#123; "autoDeploy": true, "readQuorum": 1, "writeQuorum": "majority", "executionMode": "asynchronous", "readYourWrites": true, "servers": &#123; "jdorientdb1": "master", "jdorietndb2":"master", "jdorientdb3":"master", "jdorientdb4":"master" &#125;, "clusters": &#123; "internal": &#123; &#125;, "*": &#123; "servers": ["jdorientdb1","jdorientdb2","jdorientdb3","jdorientdb4"] &#125; &#125;&#125; step 7:每台虚拟机分别设置ip别名，修改/etc/hosts123410.14.133.18 jdorientdb210.14.133.13 jdorientdb110.14.133.25 jdorientdb310.14.133.75 jdorientdb4 step 8:将配置好的orientdb分别发送到其他虚拟机1scp -r /software/orientdb jdorientdb@jdorientdb2:/software/jdorientdb step 9:分别启动每台虚拟机上的orientdb进程123nohup bin/dserver.sh &gt; tmp.log &amp;首次启动需要设置nodeName，设置为与ip别名一致 Reference:1.Distributed-Configuration2.http://blog.topspeedsnail.com/archives/18843.Performance-Tuning]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[已有集群环境中运行不同版本的spark任务]]></title>
    <url>%2F2016%2F10%2F09%2Fspark-run-spark-on-diff-env%2F</url>
    <content type="text"><![CDATA[因为公司部署的Spark集群版本还停留在1.5.2，但现在spark已经更新到2.0.1了。由于想迫切尝试下spark的新特性，如Spark ML模型保存功能,SparkSession统一接口 etc. 因此想到是否可以基于现有的集群环境来运行最新版本spark程序，经过几番捣腾，终于成功，固记录下来分享给大家。 本文使用的spark版本是2.0.0，而公司的集群spark版本是1.5.2 1. 首先去官网http://spark.apache.org/下载最新版本包，也可去github上clone最新源码，然后在IDEA中编译 step1 由于spark2.0取消了spark-assembly jar，但2.0之前打包方式均为assembly jar形式；如果还想使用assembly jar形式，可以修改源码assembly模块中pom.xml配置: 12345678&lt;properties&gt; &lt;sbt.project.name&gt;assembly&lt;/sbt.project.name&gt; &lt;build.testJarPhase&gt;none&lt;/build.testJarPhase&gt; &lt;build.copyDependenciesPhase&gt;package&lt;/build.copyDependenciesPhase&gt; &lt;!--fat jar config by sj_mei--&gt; &lt;spark.jar.basename&gt;spark-assembly-$&#123;project.version&#125;-hadoop$&#123;hadoop.version&#125;.jar&lt;/spark.jar.basename&gt; &lt;spark.jar&gt;$&#123;project.build.directory&#125;/$&#123;spark.jar.basename&#125;&lt;/spark.jar&gt;&lt;/properties&gt; step2 spark解压后目录： 2. 因spark-shell,spark-sql,spark-submit运行时需要读取SPARK_HOME与SPARK_CONF_DIR配置，为让spark不用原生产环境中的配置，所以需要重新设置零时环境变量(对当前会话窗口有效)1234567方法1：在自定义env配置文件.mybashrc中设置：export SPARK_HOME=/home/sjmei/plugins/spark_2.0export SPARK_CONF_DIR=/home/sjmei/plugins/spark_2.0/conf让当前设置生效:source .mybashrc方法2：也可以在bin目录下spark-shell.sh,spark-submit.sh中单独配置上述两个变量 3. 修改conf/spark-defaults.conf文件，避免让spark运行时读取系统原有配置文件1234spark.yarn.jars hdfs://ns2/user/mart_risk/jrdm/jars/*.jaror[运行时会警告参数已弃用]spark.yarn.jar hdfs://ns2/user/mart_risk/sjmei/lib/spark-assembly-2.0.0-hadoop2.7.1.jar 4. 经过上述同步，spark2.0就设置好了，接下来就可以试试运行bin/spark-shell，bin/spark-submit来提交任务了^_^]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OrientDB单机版安装笔记]]></title>
    <url>%2F2016%2F09%2F28%2Forientdb-install%2F</url>
    <content type="text"><![CDATA[step one:设置ORIENTDB_HOME in ~/.bash_rc1export ORIENTDB_HOME="your soft directory" step two:修改config/orientdb-server-config.xml，设置用户登录密码1234&lt;users&gt; &lt;user resources="*" password="orientdb" name="orientdb"/&gt; &lt;user resources="connect,server.listDatabases,server.dblist" password="spark" name="guest"/&gt;&lt;/users&gt; step three:修改bin/orientdb.shorientdb启动默认需要root权限，修改配置去除root权限限制1234567891011# You have to SET the OrientDB installation directory hereORIENTDB_DIR="$ORIENTDB_HOME"ORIENTDB_USER="orientdb"start方法修改为：#su $ORIENTDB_USER -c "cd \"$ORIENTDB_DIR/bin\"; /usr/bin/nohup ./server.sh 1&gt;$LOG_DIR/orientdb.log 2&gt;$LOG_DIR/orientdb.err &amp;"/usr/bin/nohup ./server.sh 1&gt;$LOG_DIR/orientdb.log 2&gt;$LOG_DIR/orientdb.err &amp;stop方法修改为：#su $ORIENTDB_USER -c "cd \"$ORIENTDB_DIR/bin\"; /usr/bin/nohup ./shutdown.sh 1&gt;&gt;$LOG_DIR/orientdb.log 2&gt;&gt;$LOG_DIR/orientdb.err &amp;"/usr/bin/nohup ./shutdown.sh 1&gt;&gt;$LOG_DIR/orientdb.log 2&gt;&gt;$LOG_DIR/orientdb.err &amp; bin/server.sh启动进程服务 or bin/orientdb.sh start/stop 后台启动服务neo4j单机节点安装：修改conf/neo4j.conf1dbms.connector.http.address=172.19.163.83:7474 Reference:1.http://blog.topspeedsnail.com/archives/1884]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>orientdb</tag>
        <tag>neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Netty Rpc]]></title>
    <url>%2F2016%2F09%2F25%2Fspark-rpc%2F</url>
    <content type="text"><![CDATA[RpcEnvRPC Environment (aka RpcEnv) is an environment for RpcEndpoints to process messages. A RPC Environment manages the entire lifecycle of RpcEndpoints: registers (sets up) endpoints (by name or uri) routes incoming messages to them stops them RpcEndpointRpcEndpoints define how to handle messages (what functions to execute given a message). RpcEndpoints register (with a name or uri) to RpcEnv to receive messages from RpcEndpointRefs.RpcEndpoints 定义了如何处理消息(对指定消息执行指定功能)，它向RpcEnv注册并接收来自RpcEndpointRefs的消息 A RpcEndpoint can be registered to one and only one RpcEnv.The lifecycle of a RpcEndpoint is onStart, receive and onStop in sequence.receive can be called concurrently.Tip: If you want receive to be thread-safe, use ThreadSafeRpcEndpoint. RpcEndpointRefA RpcEndpointRef is a reference for a RpcEndpoint in a RpcEnv.It is serializable entity and so you can send it over a network or save it for later use (it can however be deserialized using the owning RpcEnv only).A RpcEndpointRef has an address (a Spark URL), and a name.You can send asynchronous one-way messages to the corresponding RpcEndpoint using send method.You can send a semi-synchronous message, i.e. “subscribe” to be notified when a response arrives, using ask method. You can also block the current calling thread for a response using askWithRetry method. RpcAddressRpcAddress is the logical address for an RPC Environment, with hostname and port.RpcAddress is encoded as a Spark URL, i.e. spark://host:port. RpcEndpointAddressRpcEndpointAddress is the logical address for an endpoint registered to an RPC Environment, with RpcAddress and name.+It is in the format of spark://[name]@[rpcAddress.host]:[rpcAddress.port]. Ask Operation TimeoutAsk operation is when a RPC client expects a response to a message. It is a blocking operation.You can control the time to wait for a response using the following settings (in that order):spark.rpc.askTimeoutspark.network.timeoutTheir value can be a number alone (seconds) or any number with time suffix, e.g. 50s, 100ms, or 250us. See Settings. Reference: mastering-apache-spark-book#spark-rpc]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Submit任务提交过程源码分析]]></title>
    <url>%2F2016%2F09%2F25%2Fspark-submit%2F</url>
    <content type="text"><![CDATA[由于对spark-submit提交后执行流程比较好奇，所以研究了一下spark源码，以下算是阅读笔记吧。 spark-submit启动脚本： shell -z判断参数是否为空； $@：表示所有参数；$?：表示上一次程序返回值 使用@ 或可以获取数组中的所有元素，例如：${array_name[]}，${array_name[@]} SparkSubmit–&gt;yarn/Client–&gt;ApplicationMaster Client：负责提交作业到Master。 Master：接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。 Worker：负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，比如启动Driver和Executor。 shell read可以带有-a, -d, -e, -n, -p, -r, -t, 和 -s八个选项。1234567-a ：将内容读入到数值中-d ：表示delimiter，即定界符，一般情况下是以IFS为参数的间隔，但是通过-d，可以定义一直读到出现执行的字符位置。-n ：用于限定最多可以有多少字符可以作为有效读入。-p ：用于给出提示符，在前面的例子中我们使用了echo –n “…“来给出提示符，可以使用read –p ‘… my promt?’value的方式只需一个语句来表示。-r ：在参数输入中，我们可以使用’/’表示没有输入完，换行继续输入。-s ：对于一些特殊的符号，例如箭头号，不将他们在terminal上打印，我们按光标，在回车之后，如果要求显示，即echo，光标向上，如果不使用-s，在输入时，输入处显示^[[A，即在terminal上打印，之后如果要求echo，光标会上移。-t ：用于表示等待输入的时间，单位为秒，等待时间超过，将继续执行后面的脚本，注意不作为null输入，参数将保留原有的值 1exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@" spark-class代码(配置spark任务执行环境变量，提交执行程序)：123456789101112131415161718192021222324. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.shbuild_command() &#123; "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@" printf "%d\0" $?&#125;//解析参数CMD=()while IFS= read -d '' -r ARG; do CMD+=("$ARG")done &lt; &lt;(build_command "$@")COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;if [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfiCMD=("$&#123;CMD[@]:0:$LAST&#125;")exec "$&#123;CMD[@]&#125;"exec "$&#123;CMD[@]&#125;"，即执行java -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.deploy.SparkSubmit "$@"，最终执行代码示例：/software/servers/jdk1.7.0_67/bin/java -cp /software/servers/druid/mart_risk/hadoop/lib/native/:/software/servers/druid/mart_risk/hadoop/share/hadoop/common/lib/hadoop-lzo-0.4.20.jar:/software/conf/druid/mart_risk/bdp_jmart_risk.bdp_jmart_risk_hkh/hive_conf/:/home/mart_risk/data_dir/sjmei/plugins/spark_2.0/conf/:/home/mart_risk/data_dir/sjmei/plugins/spark_2.0/jars/*:/software/conf/druid/mart_risk/bdp_jmart_risk.bdp_jmart_risk_hkh/hadoop_conf/ -XX:MaxPermSize=256m org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode cluster --conf spark.driver.memory=10g --properties-file ./conf/spark-defaults.conf --class com.jd.risk.dm.spark.ml.alphago.GBTMLlib --name spark gbt algo for devices --num-executors 10 --executor-memory 10g --executor-cores 2 --jars ./examples/jars/scopt_2.11-3.3.0.jar --queue bdp_jmart_risk.bdp_jmart_risk_hkh ./libs/jrdm-dm-2.0-SNAPSHOT.jar hdfs://ns2/user/mart_risk/dev.db/risk_jrdm_msj_devices_training_black training deploy.SparkSubmit代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448def main(args: Array[String]): Unit = &#123; val appArgs = new SparkSubmitArguments(args) ... appArgs.action match &#123; case SparkSubmitAction.SUBMIT =&gt; submit(appArgs) case SparkSubmitAction.KILL =&gt; kill(appArgs) case SparkSubmitAction.REQUEST_STATUS =&gt; requestStatus(appArgs) &#125; &#125; /** * Submit the application using the provided parameters. * This runs in two steps. First, we prepare the launch environment by setting up * the appropriate classpath, system properties, and application arguments for * running the child main class based on the cluster manager and the deploy mode. * Second, we use this launch environment to invoke the main method of the child * main class. */ @tailrec private def submit(args: SparkSubmitArguments): Unit = &#123; val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args) def doRunMain(): Unit = &#123; if (args.proxyUser != null) &#123; val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser, UserGroupInformation.getCurrentUser()) try &#123; proxyUser.doAs(new PrivilegedExceptionAction[Unit]() &#123; override def run(): Unit = &#123; runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) &#125; &#125;) &#125; catch &#123; ... &#125; &#125; else &#123; runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose) &#125; &#125; // In standalone cluster mode, there are two submission gateways: // (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper // (2) The new REST-based gateway introduced in Spark 1.3 // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over // to use the legacy gateway if the master endpoint turns out to be not a REST server. if (args.isStandaloneCluster &amp;&amp; args.useRest) &#123; try &#123; // scalastyle:off println printStream.println("Running Spark using the REST application submission protocol.") // scalastyle:on println doRunMain() &#125; catch &#123; ... args.useRest = false submit(args) &#125; // In all other modes, just run the main class as prepared &#125; else &#123; doRunMain() &#125; &#125; /** * Prepare the environment for submitting an application. * This returns a 4-tuple: * (1) the arguments for the child process, * (2) a list of classpath entries for the child, * (3) a map of system properties, and * (4) the main class for the child * Exposed for testing. */ private[deploy] def prepareSubmitEnvironment(args: SparkSubmitArguments) : (Seq[String], Seq[String], Map[String, String], String) = &#123; // Return values val childArgs = new ArrayBuffer[String]() val childClasspath = new ArrayBuffer[String]() val sysProps = new HashMap[String, String]() var childMainClass = "" // Set the cluster manager val clusterManager: Int = args.master match &#123; case "yarn" =&gt; YARN case "yarn-client" | "yarn-cluster" =&gt; printWarning(s"Master $&#123;args.master&#125; is deprecated since 2.0." + " Please use master \"yarn\" with specified deploy mode instead.") YARN case m if m.startsWith("spark") =&gt; STANDALONE case m if m.startsWith("mesos") =&gt; MESOS case m if m.startsWith("local") =&gt; LOCAL case _ =&gt; printErrorAndExit("Master must either be yarn or start with spark, mesos, local") -1 &#125; // Set the deploy mode; default is client mode var deployMode: Int = args.deployMode match &#123; case "client" | null =&gt; CLIENT case "cluster" =&gt; CLUSTER case _ =&gt; printErrorAndExit("Deploy mode must be either client or cluster"); -1 &#125; // Because the deprecated way of specifying "yarn-cluster" and "yarn-client" encapsulate both // the master and deploy mode, we have some logic to infer the master and deploy mode // from each other if only one is specified, or exit early if they are at odds. if (clusterManager == YARN) &#123; (args.master, args.deployMode) match &#123; case ("yarn-cluster", null) =&gt; deployMode = CLUSTER args.master = "yarn" case ("yarn-cluster", "client") =&gt; printErrorAndExit("Client deploy mode is not compatible with master \"yarn-cluster\"") case ("yarn-client", "cluster") =&gt; printErrorAndExit("Cluster deploy mode is not compatible with master \"yarn-client\"") case (_, mode) =&gt; args.master = "yarn" &#125; // Make sure YARN is included in our build if we're trying to use it if (!Utils.classIsLoadable("org.apache.spark.deploy.yarn.Client") &amp;&amp; !Utils.isTesting) &#123; printErrorAndExit( "Could not load YARN classes. " + "This copy of Spark may not have been compiled with YARN support.") &#125; &#125; // Update args.deployMode if it is null. It will be passed down as a Spark property later. (args.deployMode, deployMode) match &#123; case (null, CLIENT) =&gt; args.deployMode = "client" case (null, CLUSTER) =&gt; args.deployMode = "cluster" case _ =&gt; &#125; val isYarnCluster = clusterManager == YARN &amp;&amp; deployMode == CLUSTER val isMesosCluster = clusterManager == MESOS &amp;&amp; deployMode == CLUSTER // Resolve maven dependencies if there are any and add classpath to jars. Add them to py-files // too for packages that include Python code val exclusions: Seq[String] = if (!StringUtils.isBlank(args.packagesExclusions)) &#123; args.packagesExclusions.split(",") &#125; else &#123; Nil &#125; val resolvedMavenCoordinates = SparkSubmitUtils.resolveMavenCoordinates(args.packages, Option(args.repositories), Option(args.ivyRepoPath), exclusions = exclusions) if (!StringUtils.isBlank(resolvedMavenCoordinates)) &#123; args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates) if (args.isPython) &#123; args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates) &#125; &#125; // install any R packages that may have been passed through --jars or --packages. // Spark Packages may contain R source code inside the jar. if (args.isR &amp;&amp; !StringUtils.isBlank(args.jars)) &#123; RPackageUtils.checkAndBuildRPackage(args.jars, printStream, args.verbose) &#125; // Require all python files to be local, so we can add them to the PYTHONPATH // In YARN cluster mode, python files are distributed as regular files, which can be non-local. // In Mesos cluster mode, non-local python files are automatically downloaded by Mesos. if (args.isPython &amp;&amp; !isYarnCluster &amp;&amp; !isMesosCluster) &#123; if (Utils.nonLocalPaths(args.primaryResource).nonEmpty) &#123; printErrorAndExit(s"Only local python files are supported: $args.primaryResource") &#125; val nonLocalPyFiles = Utils.nonLocalPaths(args.pyFiles).mkString(",") if (nonLocalPyFiles.nonEmpty) &#123; printErrorAndExit(s"Only local additional python files are supported: $nonLocalPyFiles") &#125; &#125; // Require all R files to be local if (args.isR &amp;&amp; !isYarnCluster) &#123; if (Utils.nonLocalPaths(args.primaryResource).nonEmpty) &#123; printErrorAndExit(s"Only local R files are supported: $args.primaryResource") &#125; &#125; // The following modes are not supported or applicable (clusterManager, deployMode) match &#123; ... &#125; // If we're running a python app, set the main class to our specific python runner if (args.isPython &amp;&amp; deployMode == CLIENT) &#123; if (args.primaryResource == PYSPARK_SHELL) &#123; args.mainClass = "org.apache.spark.api.python.PythonGatewayServer" &#125; else &#123; // If a python file is provided, add it to the child arguments and list of files to deploy. // Usage: PythonAppRunner &lt;main python file&gt; &lt;extra python files&gt; [app arguments] args.mainClass = "org.apache.spark.deploy.PythonRunner" args.childArgs = ArrayBuffer(args.primaryResource, args.pyFiles) ++ args.childArgs if (clusterManager != YARN) &#123; // The YARN backend distributes the primary file differently, so don't merge it. args.files = mergeFileLists(args.files, args.primaryResource) &#125; &#125; if (clusterManager != YARN) &#123; // The YARN backend handles python files differently, so don't merge the lists. args.files = mergeFileLists(args.files, args.pyFiles) &#125; if (args.pyFiles != null) &#123; sysProps("spark.submit.pyFiles") = args.pyFiles &#125; &#125; // In YARN mode for an R app, add the SparkR package archive and the R package // archive containing all of the built R libraries to archives so that they can // be distributed with the job ... // Special flag to avoid deprecation warnings at the client sysProps("SPARK_SUBMIT") = "true" // A list of rules to map each argument to system properties or command-line options in // each deploy mode; we iterate through these below val options = List[OptionAssigner]( // All cluster managers OptionAssigner(args.master, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.master"), OptionAssigner(args.deployMode, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.submit.deployMode"), OptionAssigner(args.name, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.app.name"), OptionAssigner(args.ivyRepoPath, ALL_CLUSTER_MGRS, CLIENT, sysProp = "spark.jars.ivy"), OptionAssigner(args.driverMemory, ALL_CLUSTER_MGRS, CLIENT, sysProp = "spark.driver.memory"), OptionAssigner(args.driverExtraClassPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.driver.extraClassPath"), OptionAssigner(args.driverExtraJavaOptions, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.driver.extraJavaOptions"), OptionAssigner(args.driverExtraLibraryPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = "spark.driver.extraLibraryPath"), // Yarn only OptionAssigner(args.queue, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.queue"), OptionAssigner(args.numExecutors, YARN, ALL_DEPLOY_MODES, sysProp = "spark.executor.instances"), OptionAssigner(args.jars, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.dist.jars"), OptionAssigner(args.files, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.dist.files"), OptionAssigner(args.archives, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.dist.archives"), OptionAssigner(args.principal, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.principal"), OptionAssigner(args.keytab, YARN, ALL_DEPLOY_MODES, sysProp = "spark.yarn.keytab"), // Other options OptionAssigner(args.executorCores, STANDALONE | YARN, ALL_DEPLOY_MODES, sysProp = "spark.executor.cores"), OptionAssigner(args.executorMemory, STANDALONE | MESOS | YARN, ALL_DEPLOY_MODES, sysProp = "spark.executor.memory"), OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS, ALL_DEPLOY_MODES, sysProp = "spark.cores.max"), OptionAssigner(args.files, LOCAL | STANDALONE | MESOS, ALL_DEPLOY_MODES, sysProp = "spark.files"), OptionAssigner(args.jars, LOCAL, CLIENT, sysProp = "spark.jars"), OptionAssigner(args.jars, STANDALONE | MESOS, ALL_DEPLOY_MODES, sysProp = "spark.jars"), OptionAssigner(args.driverMemory, STANDALONE | MESOS | YARN, CLUSTER, sysProp = "spark.driver.memory"), OptionAssigner(args.driverCores, STANDALONE | MESOS | YARN, CLUSTER, sysProp = "spark.driver.cores"), OptionAssigner(args.supervise.toString, STANDALONE | MESOS, CLUSTER, sysProp = "spark.driver.supervise"), OptionAssigner(args.ivyRepoPath, STANDALONE, CLUSTER, sysProp = "spark.jars.ivy") ) // In client mode, launch the application main class directly // In addition, add the main application jar and any added jars (if any) to the classpath if (deployMode == CLIENT) &#123; childMainClass = args.mainClass if (isUserJar(args.primaryResource)) &#123; childClasspath += args.primaryResource &#125; if (args.jars != null) &#123; childClasspath ++= args.jars.split(",") &#125; if (args.childArgs != null) &#123; childArgs ++= args.childArgs &#125; &#125; // Map all arguments to command-line options or system properties for our chosen mode for (opt &lt;- options) &#123; if (opt.value != null &amp;&amp; (deployMode &amp; opt.deployMode) != 0 &amp;&amp; (clusterManager &amp; opt.clusterManager) != 0) &#123; if (opt.clOption != null) &#123; childArgs += (opt.clOption, opt.value) &#125; if (opt.sysProp != null) &#123; sysProps.put(opt.sysProp, opt.value) &#125; &#125; &#125; // Add the application jar automatically so the user doesn't have to call sc.addJar // For YARN cluster mode, the jar is already distributed on each node as "app.jar" // For python and R files, the primary resource is already distributed as a regular file if (!isYarnCluster &amp;&amp; !args.isPython &amp;&amp; !args.isR) &#123; var jars = sysProps.get("spark.jars").map(x =&gt; x.split(",").toSeq).getOrElse(Seq.empty) if (isUserJar(args.primaryResource)) &#123; jars = jars ++ Seq(args.primaryResource) &#125; sysProps.put("spark.jars", jars.mkString(",")) &#125; // In standalone cluster mode, use the REST client to submit the application (Spark 1.3+). // All Spark parameters are expected to be passed to the client through system properties. if (args.isStandaloneCluster) &#123; if (args.useRest) &#123; childMainClass = "org.apache.spark.deploy.rest.RestSubmissionClient" childArgs += (args.primaryResource, args.mainClass) &#125; else &#123; // In legacy standalone cluster mode, use Client as a wrapper around the user class childMainClass = "org.apache.spark.deploy.Client" if (args.supervise) &#123; childArgs += "--supervise" &#125; Option(args.driverMemory).foreach &#123; m =&gt; childArgs += ("--memory", m) &#125; Option(args.driverCores).foreach &#123; c =&gt; childArgs += ("--cores", c) &#125; childArgs += "launch" childArgs += (args.master, args.primaryResource, args.mainClass) &#125; if (args.childArgs != null) &#123; childArgs ++= args.childArgs &#125; &#125; // Let YARN know it's a pyspark app, so it distributes needed libraries. if (clusterManager == YARN) &#123; if (args.isPython) &#123; sysProps.put("spark.yarn.isPython", "true") &#125; if (args.pyFiles != null) &#123; sysProps("spark.submit.pyFiles") = args.pyFiles &#125; &#125; // assure a keytab is available from any place in a JVM if (clusterManager == YARN || clusterManager == LOCAL) &#123; if (args.principal != null) &#123; require(args.keytab != null, "Keytab must be specified when principal is specified") if (!new File(args.keytab).exists()) &#123; throw new SparkException(s"Keytab file: $&#123;args.keytab&#125; does not exist") &#125; else &#123; // Add keytab and principal configurations in sysProps to make them available // for later use; e.g. in spark sql, the isolated class loader used to talk // to HiveMetastore will use these settings. They will be set as Java system // properties and then loaded by SparkConf sysProps.put("spark.yarn.keytab", args.keytab) sysProps.put("spark.yarn.principal", args.principal) UserGroupInformation.loginUserFromKeytab(args.principal, args.keytab) &#125; &#125; &#125; // In yarn-cluster mode, use yarn.Client as a wrapper around the user class if (isYarnCluster) &#123; childMainClass = "org.apache.spark.deploy.yarn.Client" if (args.isPython) &#123; childArgs += ("--primary-py-file", args.primaryResource) childArgs += ("--class", "org.apache.spark.deploy.PythonRunner") &#125; else if (args.isR) &#123; val mainFile = new Path(args.primaryResource).getName childArgs += ("--primary-r-file", mainFile) childArgs += ("--class", "org.apache.spark.deploy.RRunner") &#125; else &#123; if (args.primaryResource != SparkLauncher.NO_RESOURCE) &#123; childArgs += ("--jar", args.primaryResource) &#125; childArgs += ("--class", args.mainClass) &#125; if (args.childArgs != null) &#123; args.childArgs.foreach &#123; arg =&gt; childArgs += ("--arg", arg) &#125; &#125; &#125; if (isMesosCluster) &#123; ... &#125; // Load any properties specified through --conf and the default properties file for ((k, v) &lt;- args.sparkProperties) &#123; sysProps.getOrElseUpdate(k, v) &#125; // Ignore invalid spark.driver.host in cluster modes. if (deployMode == CLUSTER) &#123; sysProps -= "spark.driver.host" &#125; // Resolve paths in certain spark properties val pathConfigs = Seq( "spark.jars", "spark.files", "spark.yarn.dist.files", "spark.yarn.dist.archives", "spark.yarn.dist.jars") pathConfigs.foreach &#123; config =&gt; // Replace old URIs with resolved URIs, if they exist sysProps.get(config).foreach &#123; oldValue =&gt; sysProps(config) = Utils.resolveURIs(oldValue) &#125; &#125; // Resolve and format python file paths properly before adding them to the PYTHONPATH. // The resolving part is redundant in the case of --py-files, but necessary if the user // explicitly sets `spark.submit.pyFiles` in his/her default properties file. sysProps.get("spark.submit.pyFiles").foreach &#123; pyFiles =&gt; val resolvedPyFiles = Utils.resolveURIs(pyFiles) val formattedPyFiles = PythonRunner.formatPaths(resolvedPyFiles).mkString(",") sysProps("spark.submit.pyFiles") = formattedPyFiles &#125; (childArgs, childClasspath, sysProps, childMainClass) &#125;/** * Run the main method of the child class using the provided launch environment. * Note that this main class will not be the one provided by the user if we * are running cluster deploy mode or python applications. */ private def runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit = &#123; if (verbose) &#123; printStream.println(s"Main class:\n$childMainClass") printStream.println(s"Arguments:\n$&#123;childArgs.mkString("\n")&#125;") printStream.println(s"System properties:\n$&#123;sysProps.mkString("\n")&#125;") printStream.println(s"Classpath elements:\n$&#123;childClasspath.mkString("\n")&#125;") printStream.println("\n") &#125; val loader = if (sysProps.getOrElse("spark.driver.userClassPathFirst", "false").toBoolean) &#123; new ChildFirstURLClassLoader(new Array[URL](0), Thread.currentThread.getContextClassLoader) &#125; else &#123; new MutableURLClassLoader(new Array[URL](0), Thread.currentThread.getContextClassLoader) &#125; Thread.currentThread.setContextClassLoader(loader) for (jar &lt;- childClasspath) &#123; addJarToClasspath(jar, loader) &#125; for ((key, value) &lt;- sysProps) &#123; System.setProperty(key, value) &#125; var mainClass: Class[_] = null try &#123; mainClass = Utils.classForName(childMainClass) &#125; catch &#123; ... &#125; deploy.yarn.Client代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317def main(argStrings: Array[String]) &#123; if (!sys.props.contains("SPARK_SUBMIT")) &#123; logWarning("WARNING: This client is deprecated and will be removed in a " + "future version of Spark. Use ./bin/spark-submit with \"--master yarn\"") &#125; // Set an env variable indicating we are running in YARN mode. // Note that any env variable with the SPARK_ prefix gets propagated to all (remote) processes System.setProperty("SPARK_YARN_MODE", "true") val sparkConf = new SparkConf val args = new ClientArguments(argStrings) new Client(args, sparkConf).run() &#125; /** * Submit an application to the ResourceManager. * If set spark.yarn.submit.waitAppCompletion to true, it will stay alive * reporting the application's status until the application has exited for any reason. * Otherwise, the client process will exit after submission. * If the application finishes with a failed, killed, or undefined status, * throw an appropriate SparkException. */ def run(): Unit = &#123; this.appId = submitApplication() if (!launcherBackend.isConnected() &amp;&amp; fireAndForget) &#123; val report = getApplicationReport(appId)//ApplicationReport是应用程序的报告（包括程序用户、程序队列、程序名称等等） val state = report.getYarnApplicationState//得到应用程序的完成状态 logInfo(s"Application report for $appId (state: $state)") logInfo(formatReportDetails(report)) if (state == YarnApplicationState.FAILED || state == YarnApplicationState.KILLED) &#123; throw new SparkException(s"Application $appId finished with status: $state") &#125; &#125; else &#123; //涉及两个对象YarnApplicationState（对于yarn来说任务的状态）、FinalApplicationStatus（对于任务来说任务的运行状态） val (yarnApplicationState, finalApplicationStatus) = monitorApplication(appId) if (yarnApplicationState == YarnApplicationState.FAILED || finalApplicationStatus == FinalApplicationStatus.FAILED) &#123; throw new SparkException(s"Application $appId finished with failed status") &#125; if (yarnApplicationState == YarnApplicationState.KILLED || finalApplicationStatus == FinalApplicationStatus.KILLED) &#123; throw new SparkException(s"Application $appId is killed") &#125; if (finalApplicationStatus == FinalApplicationStatus.UNDEFINED) &#123; throw new SparkException(s"The final status of application $appId is undefined") &#125; &#125; &#125; /** * Submit an application running our ApplicationMaster to the ResourceManager. * The stable Yarn API provides a convenience method (YarnClient#createApplication) for * creating applications and setting up the application submission context. This was not * available in the alpha API. */ def submitApplication(): ApplicationId = &#123; var appId: ApplicationId = null try &#123; launcherBackend.connect() // Setup the credentials before doing anything else, // so we have don't have issues at any point. setupCredentials() yarnClient.init(yarnConf) yarnClient.start() logInfo("Requesting a new application from cluster with %d NodeManagers" .format(yarnClient.getYarnClusterMetrics.getNumNodeManagers)) // Get a new application from our RM val newApp = yarnClient.createApplication()// 构建ApplicationMaster的container(包括jar包路径 userClas等) val newAppResponse = newApp.getNewApplicationResponse() appId = newAppResponse.getApplicationId() reportLauncherState(SparkAppHandle.State.SUBMITTED) launcherBackend.setAppId(appId.toString) // Verify whether the cluster has enough resources for our AM verifyClusterResources(newAppResponse) // Set up the appropriate contexts to launch our AM val containerContext = createContainerLaunchContext(newAppResponse) val appContext = createApplicationSubmissionContext(newApp, containerContext) // Finally, submit and monitor the application logInfo(s"Submitting application $appId to ResourceManager") yarnClient.submitApplication(appContext) appId &#125; catch &#123; ... &#125; &#125;/** * Set up a ContainerLaunchContext to launch our ApplicationMaster container. * This sets up the launch environment, java options, and the command for launching the AM. */ private def createContainerLaunchContext(newAppResponse: GetNewApplicationResponse) : ContainerLaunchContext = &#123; logInfo("Setting up container launch context for our AM") val appId = newAppResponse.getApplicationId val appStagingDirPath = new Path(appStagingBaseDir, getAppStagingDir(appId)) val pySparkArchives = if (sparkConf.get(IS_PYTHON_APP)) &#123; findPySparkArchives() &#125; else &#123; Nil &#125; val launchEnv = setupLaunchEnv(appStagingDirPath, pySparkArchives) val localResources = prepareLocalResources(appStagingDirPath, pySparkArchives) val amContainer = Records.newRecord(classOf[ContainerLaunchContext]) amContainer.setLocalResources(localResources.asJava) amContainer.setEnvironment(launchEnv.asJava) val javaOpts = ListBuffer[String]() // Set the environment variable through a command prefix // to append to the existing value of the variable var prefixEnv: Option[String] = None // Add Xmx for AM memory javaOpts += "-Xmx" + amMemory + "m" val tmpDir = new Path( YarnSparkHadoopUtil.expandEnvironment(Environment.PWD), YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR ) javaOpts += "-Djava.io.tmpdir=" + tmpDir // TODO: Remove once cpuset version is pushed out. // The context is, default gc for server class machines ends up using all cores to do gc - // hence if there are multiple containers in same node, Spark GC affects all other containers' // performance (which can be that of other Spark containers) // Instead of using this, rely on cpusets by YARN to enforce "proper" Spark behavior in // multi-tenant environments. Not sure how default Java GC behaves if it is limited to subset // of cores on a node. val useConcurrentAndIncrementalGC = launchEnv.get("SPARK_USE_CONC_INCR_GC").exists(_.toBoolean) if (useConcurrentAndIncrementalGC) &#123; // In our expts, using (default) throughput collector has severe perf ramifications in // multi-tenant machines javaOpts += "-XX:+UseConcMarkSweepGC" javaOpts += "-XX:MaxTenuringThreshold=31" javaOpts += "-XX:SurvivorRatio=8" javaOpts += "-XX:+CMSIncrementalMode" javaOpts += "-XX:+CMSIncrementalPacing" javaOpts += "-XX:CMSIncrementalDutyCycleMin=0" javaOpts += "-XX:CMSIncrementalDutyCycle=10" &#125; // Include driver-specific java options if we are launching a driver if (isClusterMode) &#123; val driverOpts = sparkConf.get(DRIVER_JAVA_OPTIONS).orElse(sys.env.get("SPARK_JAVA_OPTS")) driverOpts.foreach &#123; opts =&gt; javaOpts ++= Utils.splitCommandString(opts).map(YarnSparkHadoopUtil.escapeForShell) &#125; val libraryPaths = Seq(sparkConf.get(DRIVER_LIBRARY_PATH), sys.props.get("spark.driver.libraryPath")).flatten if (libraryPaths.nonEmpty) &#123; prefixEnv = Some(getClusterPath(sparkConf, Utils.libraryPathEnvPrefix(libraryPaths))) &#125; if (sparkConf.get(AM_JAVA_OPTIONS).isDefined) &#123; logWarning(s"$&#123;AM_JAVA_OPTIONS.key&#125; will not take effect in cluster mode") &#125; &#125; else &#123; // Validate and include yarn am specific java options in yarn-client mode. sparkConf.get(AM_JAVA_OPTIONS).foreach &#123; opts =&gt; if (opts.contains("-Dspark")) &#123; val msg = s"$&#123;AM_JAVA_OPTIONS.key&#125; is not allowed to set Spark options (was '$opts')." throw new SparkException(msg) &#125; if (opts.contains("-Xmx")) &#123; val msg = s"$&#123;AM_JAVA_OPTIONS.key&#125; is not allowed to specify max heap memory settings " + s"(was '$opts'). Use spark.yarn.am.memory instead." throw new SparkException(msg) &#125; javaOpts ++= Utils.splitCommandString(opts).map(YarnSparkHadoopUtil.escapeForShell) &#125; sparkConf.get(AM_LIBRARY_PATH).foreach &#123; paths =&gt; prefixEnv = Some(getClusterPath(sparkConf, Utils.libraryPathEnvPrefix(Seq(paths)))) &#125; &#125; // For log4j configuration to reference javaOpts += ("-Dspark.yarn.app.container.log.dir=" + ApplicationConstants.LOG_DIR_EXPANSION_VAR) YarnCommandBuilderUtils.addPermGenSizeOpt(javaOpts) val userClass = if (isClusterMode) &#123; Seq("--class", YarnSparkHadoopUtil.escapeForShell(args.userClass)) &#125; else &#123; Nil &#125; val userJar = if (args.userJar != null) &#123; Seq("--jar", args.userJar) &#125; else &#123; Nil &#125; val primaryPyFile = if (isClusterMode &amp;&amp; args.primaryPyFile != null) &#123; Seq("--primary-py-file", new Path(args.primaryPyFile).getName()) &#125; else &#123; Nil &#125; val primaryRFile = if (args.primaryRFile != null) &#123; Seq("--primary-r-file", args.primaryRFile) &#125; else &#123; Nil &#125; val amClass = if (isClusterMode) &#123; Utils.classForName("org.apache.spark.deploy.yarn.ApplicationMaster").getName &#125; else &#123; Utils.classForName("org.apache.spark.deploy.yarn.ExecutorLauncher").getName &#125; if (args.primaryRFile != null &amp;&amp; args.primaryRFile.endsWith(".R")) &#123; args.userArgs = ArrayBuffer(args.primaryRFile) ++ args.userArgs &#125; val userArgs = args.userArgs.flatMap &#123; arg =&gt; Seq("--arg", YarnSparkHadoopUtil.escapeForShell(arg)) &#125; val amArgs = Seq(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++ Seq( "--properties-file", buildPath(YarnSparkHadoopUtil.expandEnvironment(Environment.PWD), LOCALIZED_CONF_DIR, SPARK_CONF_FILE)) // Command for the ApplicationMaster val commands = prefixEnv ++ Seq( YarnSparkHadoopUtil.expandEnvironment(Environment.JAVA_HOME) + "/bin/java", "-server" ) ++ javaOpts ++ amArgs ++ Seq( "1&gt;", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout", "2&gt;", ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr") // TODO: it would be nicer to just make sure there are no null commands here val printableCommands = commands.map(s =&gt; if (s == null) "null" else s).toList amContainer.setCommands(printableCommands.asJava) ... // send the acl settings into YARN to control who has access via YARN interfaces val securityManager = new SecurityManager(sparkConf) amContainer.setApplicationACLs( YarnSparkHadoopUtil.getApplicationAclsForYarn(securityManager).asJava) setupSecurityToken(amContainer) amContainer &#125; /** * Set up the context for submitting our ApplicationMaster. * This uses the YarnClientApplication not available in the Yarn alpha API. */ def createApplicationSubmissionContext( newApp: YarnClientApplication, containerContext: ContainerLaunchContext): ApplicationSubmissionContext = &#123; val appContext = newApp.getApplicationSubmissionContext appContext.setApplicationName(sparkConf.get("spark.app.name", "Spark")) appContext.setQueue(sparkConf.get(QUEUE_NAME)) appContext.setAMContainerSpec(containerContext) appContext.setApplicationType("SPARK") sparkConf.get(APPLICATION_TAGS).foreach &#123; tags =&gt; try &#123; // The setApplicationTags method was only introduced in Hadoop 2.4+, so we need to use // reflection to set it, printing a warning if a tag was specified but the YARN version // doesn't support it. val method = appContext.getClass().getMethod( "setApplicationTags", classOf[java.util.Set[String]]) method.invoke(appContext, new java.util.HashSet[String](tags.asJava)) &#125; catch &#123; ... &#125; &#125; sparkConf.get(MAX_APP_ATTEMPTS) match &#123; case Some(v) =&gt; appContext.setMaxAppAttempts(v) case None =&gt; logDebug(s"$&#123;MAX_APP_ATTEMPTS.key&#125; is not set. " + "Cluster's default value will be used.") &#125; sparkConf.get(AM_ATTEMPT_FAILURE_VALIDITY_INTERVAL_MS).foreach &#123; interval =&gt; try &#123; val method = appContext.getClass().getMethod( "setAttemptFailuresValidityInterval", classOf[Long]) method.invoke(appContext, interval: java.lang.Long) &#125; catch &#123; ... &#125; &#125; val capability = Records.newRecord(classOf[Resource]) capability.setMemory(amMemory + amMemoryOverhead) capability.setVirtualCores(amCores) sparkConf.get(AM_NODE_LABEL_EXPRESSION) match &#123; case Some(expr) =&gt; try &#123; val amRequest = Records.newRecord(classOf[ResourceRequest]) amRequest.setResourceName(ResourceRequest.ANY) amRequest.setPriority(Priority.newInstance(0)) amRequest.setCapability(capability) amRequest.setNumContainers(1) val method = amRequest.getClass.getMethod("setNodeLabelExpression", classOf[String]) method.invoke(amRequest, expr) val setResourceRequestMethod = appContext.getClass.getMethod("setAMContainerResourceRequest", classOf[ResourceRequest]) setResourceRequestMethod.invoke(appContext, amRequest) &#125; catch &#123; ... &#125; case None =&gt; appContext.setResource(capability) &#125; appContext &#125; yarn.client.api.impl.YarnClientImpl代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public YarnClientApplication createApplication() throws YarnException, IOException &#123; ApplicationSubmissionContext context = (ApplicationSubmissionContext)Records.newRecord(ApplicationSubmissionContext.class); GetNewApplicationResponse newApp = this.getNewApplication(); ApplicationId appId = newApp.getApplicationId(); context.setApplicationId(appId); return new YarnClientApplication(newApp, context); &#125;ApplicationId getNewApplicationId() &#123; ApplicationId applicationId = BuilderUtils.newApplicationId(this.recordFactory, ResourceManager.getClusterTimeStamp(), this.applicationCounter.incrementAndGet()); LOG.info("Allocated new applicationId: " + applicationId.getId()); return applicationId; &#125; public ApplicationId submitApplication(ApplicationSubmissionContext appContext) throws YarnException, IOException &#123; ApplicationId applicationId = appContext.getApplicationId(); if(applicationId == null) &#123; throw new ApplicationIdNotProvidedException("ApplicationId is not provided in ApplicationSubmissionContext"); &#125; else &#123; SubmitApplicationRequest request = (SubmitApplicationRequest)Records.newRecord(SubmitApplicationRequest.class); request.setApplicationSubmissionContext(appContext); if(this.isSecurityEnabled() &amp;&amp; this.timelineServiceEnabled) &#123; this.addTimelineDelegationToken(appContext.getAMContainerSpec()); &#125; this.rmClient.submitApplication(request); int pollCount = 0; long startTime = System.currentTimeMillis(); EnumSet waitingStates = EnumSet.of(YarnApplicationState.NEW, YarnApplicationState.NEW_SAVING, YarnApplicationState.SUBMITTED); EnumSet failToSubmitStates = EnumSet.of(YarnApplicationState.FAILED, YarnApplicationState.KILLED); while(true) &#123; while(true) &#123; try &#123; ApplicationReport ex = this.getApplicationReport(applicationId); YarnApplicationState state = ex.getYarnApplicationState(); if(!waitingStates.contains(state)) &#123; if(failToSubmitStates.contains(state)) &#123; throw new YarnException("Failed to submit " + applicationId + " to YARN : " + ex.getDiagnostics()); &#125; LOG.info("Submitted application " + applicationId); return applicationId; &#125; long elapsedMillis = System.currentTimeMillis() - startTime; if(this.enforceAsyncAPITimeout() &amp;&amp; elapsedMillis &gt;= this.asyncApiPollTimeoutMillis) &#123; throw new YarnException("Timed out while waiting for application " + applicationId + " to be submitted successfully"); &#125; ++pollCount; if(pollCount % 10 == 0) &#123; LOG.info("Application submission is not finished, submitted application " + applicationId + " is still in " + state); &#125; try &#123; Thread.sleep(this.submitPollIntervalMillis); &#125; catch (InterruptedException var14) &#123; LOG.error("Interrupted while waiting for application " + applicationId + " to be successfully submitted."); &#125; &#125; catch (ApplicationNotFoundException var15) &#123; LOG.info("Re-submit application " + applicationId + "with the " + "same ApplicationSubmissionContext"); this.rmClient.submitApplication(request); &#125; &#125; &#125; &#125; &#125; public SubmitApplicationResponse submitApplication(SubmitApplicationRequest request) throws YarnException &#123; ApplicationSubmissionContext submissionContext = request.getApplicationSubmissionContext(); ApplicationId applicationId = submissionContext.getApplicationId(); String user = null; try &#123; user = UserGroupInformation.getCurrentUser().getShortUserName(); &#125; catch (IOException var7) &#123; LOG.warn("Unable to get the current user.", var7); RMAuditLogger.logFailure(user, "Submit Application Request", var7.getMessage(), "ClientRMService", "Exception in submitting application", applicationId); throw RPCUtil.getRemoteException(var7); &#125; if(this.rmContext.getRMApps().get(applicationId) != null) &#123; LOG.info("This is an earlier submitted application: " + applicationId); return SubmitApplicationResponse.newInstance(); &#125; else &#123; if(submissionContext.getQueue() == null) &#123; submissionContext.setQueue("default"); &#125; if(submissionContext.getApplicationName() == null) &#123; submissionContext.setApplicationName("N/A"); &#125; if(submissionContext.getApplicationType() == null) &#123; submissionContext.setApplicationType("YARN"); &#125; else if(submissionContext.getApplicationType().length() &gt; 20) &#123; submissionContext.setApplicationType(submissionContext.getApplicationType().substring(0, 20)); &#125; try &#123; this.rmAppManager.submitApplication(submissionContext, System.currentTimeMillis(), user); LOG.info("Application with id " + applicationId.getId() + " submitted by user " + user); RMAuditLogger.logSuccess(user, "Submit Application Request", "ClientRMService", applicationId); &#125; catch (YarnException var6) &#123; LOG.info("Exception in submitting application with id " + applicationId.getId(), var6); RMAuditLogger.logFailure(user, "Submit Application Request", var6.getMessage(), "ClientRMService", "Exception in submitting application", applicationId); throw var6; &#125; SubmitApplicationResponse response = (SubmitApplicationResponse)this.recordFactory.newRecordInstance(SubmitApplicationResponse.class); return response; &#125; &#125;protected void submitApplication(ApplicationSubmissionContext submissionContext, long submitTime, String user) throws YarnException &#123; ApplicationId applicationId = submissionContext.getApplicationId(); RMAppImpl application = this.createAndPopulateNewRMApp(submissionContext, submitTime, user, false); ApplicationId appId = submissionContext.getApplicationId(); if(UserGroupInformation.isSecurityEnabled()) &#123; try &#123; this.rmContext.getDelegationTokenRenewer().addApplicationAsync(appId, this.parseCredentials(submissionContext), submissionContext.getCancelTokensWhenComplete(), application.getUser()); &#125; catch (Exception var9) &#123; LOG.warn("Unable to parse credentials.", var9); assert application.getState() == RMAppState.NEW; this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED, var9.getMessage())); throw RPCUtil.getRemoteException(var9); &#125; &#125; else &#123; this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(applicationId, RMAppEventType.START)); &#125; &#125; deploy.yarn.ApplicationMaster代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234def main(args: Array[String]): Unit = &#123; SignalUtils.registerLogger(log) val amArgs = new ApplicationMasterArguments(args) // Load the properties file with the Spark configuration and set entries as system properties, // so that user code run inside the AM also has access to them. // Note: we must do this before SparkHadoopUtil instantiated if (amArgs.propertiesFile != null) &#123; Utils.getPropertiesFromFile(amArgs.propertiesFile).foreach &#123; case (k, v) =&gt; sys.props(k) = v &#125; &#125; SparkHadoopUtil.get.runAsSparkUser &#123; () =&gt; master = new ApplicationMaster(amArgs, new YarnRMClient) System.exit(master.run()) &#125; &#125; final def run(): Int = &#123; try &#123; val appAttemptId = client.getAttemptId() if (isClusterMode) &#123; // Set the web ui port to be ephemeral for yarn so we don't conflict with // other spark processes running on the same box System.setProperty("spark.ui.port", "0") // Set the master and deploy mode property to match the requested mode. System.setProperty("spark.master", "yarn") System.setProperty("spark.submit.deployMode", "cluster") // Set this internal configuration if it is running on cluster mode, this // configuration will be checked in SparkContext to avoid misuse of yarn cluster mode. System.setProperty("spark.yarn.app.id", appAttemptId.getApplicationId().toString()) &#125; logInfo("ApplicationAttemptId: " + appAttemptId) val fs = FileSystem.get(yarnConf) // This shutdown hook should run *after* the SparkContext is shut down. val priority = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY - 1 ShutdownHookManager.addShutdownHook(priority) &#123; () =&gt; val maxAppAttempts = client.getMaxRegAttempts(sparkConf, yarnConf) val isLastAttempt = client.getAttemptId().getAttemptId() &gt;= maxAppAttempts if (!finished) &#123; // The default state of ApplicationMaster is failed if it is invoked by shut down hook. // This behavior is different compared to 1.x version. // If user application is exited ahead of time by calling System.exit(N), here mark // this application as failed with EXIT_EARLY. For a good shutdown, user shouldn't call // System.exit(0) to terminate the application. finish(finalStatus, ApplicationMaster.EXIT_EARLY, "Shutdown hook called before final status was reported.") &#125; if (!unregistered) &#123; // we only want to unregister if we don't want the RM to retry if (finalStatus == FinalApplicationStatus.SUCCEEDED || isLastAttempt) &#123; unregister(finalStatus, finalMsg) cleanupStagingDir(fs) &#125; &#125; &#125; // Call this to force generation of secret so it gets populated into the // Hadoop UGI. This has to happen before the startUserApplication which does a // doAs in order for the credentials to be passed on to the executor containers. val securityMgr = new SecurityManager(sparkConf) // If the credentials file config is present, we must periodically renew tokens. So create // a new AMDelegationTokenRenewer if (sparkConf.contains(CREDENTIALS_FILE_PATH.key)) &#123; delegationTokenRenewerOption = Some(new AMDelegationTokenRenewer(sparkConf, yarnConf)) // If a principal and keytab have been set, use that to create new credentials for executors // periodically delegationTokenRenewerOption.foreach(_.scheduleLoginFromKeytab()) &#125; if (isClusterMode) &#123; runDriver(securityMgr) &#125; else &#123; runExecutorLauncher(securityMgr) &#125; &#125; catch &#123; case e: Exception =&gt; // catch everything else if not specifically handled logError("Uncaught exception: ", e) finish(FinalApplicationStatus.FAILED, ApplicationMaster.EXIT_UNCAUGHT_EXCEPTION, "Uncaught exception: " + e) &#125; exitCode &#125; private def runDriver(securityMgr: SecurityManager): Unit = &#123; addAmIpFilter() userClassThread = startUserApplication() // This a bit hacky, but we need to wait until the spark.driver.port property has // been set by the Thread executing the user class. val sc = waitForSparkContextInitialized() // If there is no SparkContext at this point, just fail the app. if (sc == null) &#123; finish(FinalApplicationStatus.FAILED, ApplicationMaster.EXIT_SC_NOT_INITED, "Timed out waiting for SparkContext.") &#125; else &#123; rpcEnv = sc.env.rpcEnv val driverRef = runAMEndpoint( sc.getConf.get("spark.driver.host"), sc.getConf.get("spark.driver.port"), isClusterMode = true) registerAM(rpcEnv, driverRef, sc.ui.map(_.appUIAddress).getOrElse(""), securityMgr) userClassThread.join() &#125; &#125;/** * Start the user class, which contains the spark driver, in a separate Thread. * If the main routine exits cleanly or exits with System.exit(N) for any N * we assume it was successful, for all other cases we assume failure. * * Returns the user thread that was started. */ private def startUserApplication(): Thread = &#123; logInfo("Starting the user application in a separate Thread") val classpath = Client.getUserClasspath(sparkConf) val urls = classpath.map &#123; entry =&gt; new URL("file:" + new File(entry.getPath()).getAbsolutePath()) &#125; val userClassLoader = if (Client.isUserClassPathFirst(sparkConf, isDriver = true)) &#123; new ChildFirstURLClassLoader(urls, Utils.getContextOrSparkClassLoader) &#125; else &#123; new MutableURLClassLoader(urls, Utils.getContextOrSparkClassLoader) &#125; var userArgs = args.userArgs if (args.primaryPyFile != null &amp;&amp; args.primaryPyFile.endsWith(".py")) &#123; // When running pyspark, the app is run using PythonRunner. The second argument is the list // of files to add to PYTHONPATH, which Client.scala already handles, so it's empty. userArgs = Seq(args.primaryPyFile, "") ++ userArgs &#125; if (args.primaryRFile != null &amp;&amp; args.primaryRFile.endsWith(".R")) &#123; // TODO(davies): add R dependencies here &#125; val mainMethod = userClassLoader.loadClass(args.userClass) .getMethod("main", classOf[Array[String]]) val userThread = new Thread &#123; override def run() &#123; try &#123; mainMethod.invoke(null, userArgs.toArray) finish(FinalApplicationStatus.SUCCEEDED, ApplicationMaster.EXIT_SUCCESS) logDebug("Done running users class") &#125; catch &#123; ... &#125; &#125; userThread.setContextClassLoader(userClassLoader) userThread.setName("Driver") userThread.start() userThread &#125;/** * Create an [[RpcEndpoint]] that communicates with the driver. * * In cluster mode, the AM and the driver belong to same process * so the AMEndpoint need not monitor lifecycle of the driver. * * @return A reference to the driver's RPC endpoint. */ private def runAMEndpoint( host: String, port: String, isClusterMode: Boolean): RpcEndpointRef = &#123; val driverEndpoint = rpcEnv.setupEndpointRef( RpcAddress(host, port.toInt), YarnSchedulerBackend.ENDPOINT_NAME) amEndpoint = rpcEnv.setupEndpoint("YarnAM", new AMEndpoint(rpcEnv, driverEndpoint, isClusterMode)) driverEndpoint &#125;private def registerAM( _rpcEnv: RpcEnv, driverRef: RpcEndpointRef, uiAddress: String, securityMgr: SecurityManager) = &#123; val sc = sparkContextRef.get() val appId = client.getAttemptId().getApplicationId().toString() val attemptId = client.getAttemptId().getAttemptId().toString() val historyAddress = sparkConf.get(HISTORY_SERVER_ADDRESS) .map &#123; text =&gt; SparkHadoopUtil.get.substituteHadoopVariables(text, yarnConf) &#125; .map &#123; address =&gt; s"$&#123;address&#125;$&#123;HistoryServer.UI_PATH_PREFIX&#125;/$&#123;appId&#125;/$&#123;attemptId&#125;" &#125; .getOrElse("") val _sparkConf = if (sc != null) sc.getConf else sparkConf val driverUrl = RpcEndpointAddress( _sparkConf.get("spark.driver.host"), _sparkConf.get("spark.driver.port").toInt, CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString allocator = client.register(driverUrl, driverRef, yarnConf, _sparkConf, uiAddress, historyAddress, securityMgr, localResources) allocator.allocateResources() reporterThread = launchReporterThread() &#125;private def runExecutorLauncher(securityMgr: SecurityManager): Unit = &#123; val port = sparkConf.getInt("spark.yarn.am.port", 0) rpcEnv = RpcEnv.create("sparkYarnAM", Utils.localHostName, port, sparkConf, securityMgr, clientMode = true) val driverRef = waitForSparkDriver() addAmIpFilter() registerAM(rpcEnv, driverRef, sparkConf.get("spark.driver.appUIAddress", ""), securityMgr) // In client mode the actor will stop the reporter thread. reporterThread.join() &#125; YarnRMClient123456789101112131415161718192021222324252627282930313233/** * Registers the application master with the RM. * * @param conf The Yarn configuration. * @param sparkConf The Spark configuration. * @param uiAddress Address of the SparkUI. * @param uiHistoryAddress Address of the application on the History Server. * @param securityMgr The security manager. * @param localResources Map with information about files distributed via YARN's cache. */ def register( driverUrl: String, driverRef: RpcEndpointRef, conf: YarnConfiguration, sparkConf: SparkConf, uiAddress: String, uiHistoryAddress: String, securityMgr: SecurityManager, localResources: Map[String, LocalResource] ): YarnAllocator = &#123; amClient = AMRMClient.createAMRMClient() amClient.init(conf) amClient.start() this.uiHistoryAddress = uiHistoryAddress logInfo("Registering the ApplicationMaster") synchronized &#123; amClient.registerApplicationMaster(Utils.localHostName(), 0, uiAddress) registered = true &#125; new YarnAllocator(driverUrl, driverRef, conf, sparkConf, amClient, getAttemptId(), securityMgr, localResources) &#125; YarnAllocator123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343/** * YarnAllocator is charged with requesting containers from the YARN ResourceManager and deciding * what to do with containers when YARN fulfills these requests. * * This class makes use of YARN's AMRMClient APIs. We interact with the AMRMClient in three ways: * * Making our resource needs known, which updates local bookkeeping about containers requested. * * Calling "allocate", which syncs our local container requests with the RM, and returns any * containers that YARN has granted to us. This also functions as a heartbeat. * * Processing the containers granted to us to possibly launch executors inside of them. * * The public methods of this class are thread-safe. All methods that mutate state are * synchronized. *//** * Request resources such that, if YARN gives us all we ask for, we'll have a number of containers * equal to maxExecutors. * * Deal with any containers YARN has granted to us by possibly launching executors in them. * * This must be synchronized because variables read in this method are mutated by other methods. */ def allocateResources(): Unit = synchronized &#123; updateResourceRequests() val progressIndicator = 0.1f // Poll the ResourceManager. This doubles as a heartbeat if there are no pending container // requests. val allocateResponse = amClient.allocate(progressIndicator) val allocatedContainers = allocateResponse.getAllocatedContainers() if (allocatedContainers.size &gt; 0) &#123; logDebug("Allocated containers: %d. Current executor count: %d. Cluster resources: %s." .format( allocatedContainers.size, numExecutorsRunning, allocateResponse.getAvailableResources)) handleAllocatedContainers(allocatedContainers.asScala) &#125; val completedContainers = allocateResponse.getCompletedContainersStatuses() if (completedContainers.size &gt; 0) &#123; logDebug("Completed %d containers".format(completedContainers.size)) processCompletedContainers(completedContainers.asScala) logDebug("Finished processing %d completed containers. Current running executor count: %d." .format(completedContainers.size, numExecutorsRunning)) &#125; &#125;/** * Handle containers granted by the RM by launching executors on them. * * Due to the way the YARN allocation protocol works, certain healthy race conditions can result * in YARN granting containers that we no longer need. In this case, we release them. * * Visible for testing. */ def handleAllocatedContainers(allocatedContainers: Seq[Container]): Unit = &#123; val containersToUse = new ArrayBuffer[Container](allocatedContainers.size) // Match incoming requests by host val remainingAfterHostMatches = new ArrayBuffer[Container] for (allocatedContainer &lt;- allocatedContainers) &#123; matchContainerToRequest(allocatedContainer, allocatedContainer.getNodeId.getHost, containersToUse, remainingAfterHostMatches) &#125; // Match remaining by rack val remainingAfterRackMatches = new ArrayBuffer[Container] for (allocatedContainer &lt;- remainingAfterHostMatches) &#123; val rack = RackResolver.resolve(conf, allocatedContainer.getNodeId.getHost).getNetworkLocation matchContainerToRequest(allocatedContainer, rack, containersToUse, remainingAfterRackMatches) &#125; // Assign remaining that are neither node-local nor rack-local val remainingAfterOffRackMatches = new ArrayBuffer[Container] for (allocatedContainer &lt;- remainingAfterRackMatches) &#123; matchContainerToRequest(allocatedContainer, ANY_HOST, containersToUse, remainingAfterOffRackMatches) &#125; if (!remainingAfterOffRackMatches.isEmpty) &#123; logDebug(s"Releasing $&#123;remainingAfterOffRackMatches.size&#125; unneeded containers that were " + s"allocated to us") for (container &lt;- remainingAfterOffRackMatches) &#123; internalReleaseContainer(container) &#125; &#125; runAllocatedContainers(containersToUse) logInfo("Received %d containers from YARN, launching executors on %d of them." .format(allocatedContainers.size, containersToUse.size)) &#125;@Override public AllocateResponse allocate(float progressIndicator) throws YarnException, IOException &#123; Preconditions.checkArgument(progressIndicator &gt;= 0, "Progress indicator should not be negative"); AllocateResponse allocateResponse = null; List&lt;ResourceRequest&gt; askList = null; List&lt;ContainerId&gt; releaseList = null; AllocateRequest allocateRequest = null; List&lt;String&gt; blacklistToAdd = new ArrayList&lt;String&gt;(); List&lt;String&gt; blacklistToRemove = new ArrayList&lt;String&gt;(); try &#123; synchronized (this) &#123; askList = new ArrayList&lt;ResourceRequest&gt;(ask.size()); for(ResourceRequest r : ask) &#123; // create a copy of ResourceRequest as we might change it while the // RPC layer is using it to send info across askList.add(ResourceRequest.newInstance(r.getPriority(), r.getResourceName(), r.getCapability(), r.getNumContainers(), r.getRelaxLocality(), r.getNodeLabelExpression())); &#125; releaseList = new ArrayList&lt;ContainerId&gt;(release); // optimistically clear this collection assuming no RPC failure ask.clear(); release.clear(); blacklistToAdd.addAll(blacklistAdditions); blacklistToRemove.addAll(blacklistRemovals); ResourceBlacklistRequest blacklistRequest = ResourceBlacklistRequest.newInstance(blacklistToAdd, blacklistToRemove); allocateRequest = AllocateRequest.newInstance(lastResponseId, progressIndicator, askList, releaseList, blacklistRequest); // clear blacklistAdditions and blacklistRemovals before // unsynchronized part blacklistAdditions.clear(); blacklistRemovals.clear(); &#125; try &#123; allocateResponse = rmClient.allocate(allocateRequest); &#125; catch (ApplicationMasterNotRegisteredException e) &#123; LOG.warn("ApplicationMaster is out of sync with ResourceManager," + " hence resyncing."); synchronized (this) &#123; release.addAll(this.pendingRelease); blacklistAdditions.addAll(this.blacklistedNodes); for (Map&lt;String, TreeMap&lt;Resource, ResourceRequestInfo&gt;&gt; rr : remoteRequestsTable .values()) &#123; for (Map&lt;Resource, ResourceRequestInfo&gt; capabalities : rr.values()) &#123; for (ResourceRequestInfo request : capabalities.values()) &#123; addResourceRequestToAsk(request.remoteRequest); &#125; &#125; &#125; &#125; // re register with RM registerApplicationMaster(); allocateResponse = allocate(progressIndicator); return allocateResponse; &#125; synchronized (this) &#123; // update these on successful RPC clusterNodeCount = allocateResponse.getNumClusterNodes(); lastResponseId = allocateResponse.getResponseId(); clusterAvailableResources = allocateResponse.getAvailableResources(); if (!allocateResponse.getNMTokens().isEmpty()) &#123; populateNMTokens(allocateResponse.getNMTokens()); &#125; if (allocateResponse.getAMRMToken() != null) &#123; updateAMRMToken(allocateResponse.getAMRMToken()); &#125; if (!pendingRelease.isEmpty() &amp;&amp; !allocateResponse.getCompletedContainersStatuses().isEmpty()) &#123; removePendingReleaseRequests(allocateResponse .getCompletedContainersStatuses()); &#125; &#125; &#125; finally &#123; // TODO how to differentiate remote yarn exception vs error in rpc if(allocateResponse == null) &#123; // we hit an exception in allocate() // preserve ask and release for next call to allocate() synchronized (this) &#123; release.addAll(releaseList); // requests could have been added or deleted during call to allocate // If requests were added/removed then there is nothing to do since // the ResourceRequest object in ask would have the actual new value. // If ask does not have this ResourceRequest then it was unchanged and // so we can add the value back safely. // This assumes that there will no concurrent calls to allocate() and // so we dont have to worry about ask being changed in the // synchronized block at the beginning of this method. for(ResourceRequest oldAsk : askList) &#123; if(!ask.contains(oldAsk)) &#123; ask.add(oldAsk); &#125; &#125; blacklistAdditions.addAll(blacklistToAdd); blacklistRemovals.addAll(blacklistToRemove); &#125; &#125; &#125; return allocateResponse; &#125;public AllocateResponse allocate(AllocateRequest request) throws YarnException, IOException &#123; AMRMTokenIdentifier amrmTokenIdentifier = this.authorizeRequest(); ApplicationAttemptId appAttemptId = amrmTokenIdentifier.getApplicationAttemptId(); ApplicationId applicationId = appAttemptId.getApplicationId(); this.amLivelinessMonitor.receivedPing(appAttemptId); ApplicationMasterService.AllocateResponseLock lock = (ApplicationMasterService.AllocateResponseLock)this.responseMap.get(appAttemptId); if(lock == null) &#123; String message = "Application attempt " + appAttemptId + " doesn\'t exist in ApplicationMasterService cache."; LOG.error(message); throw new ApplicationAttemptNotFoundException(message); &#125; else &#123; synchronized(lock) &#123; AllocateResponse lastResponse = lock.getAllocateResponse(); String filteredProgress1; if(!this.hasApplicationMasterRegistered(appAttemptId)) &#123; filteredProgress1 = "AM is not registered for known application attempt: " + appAttemptId + " or RM had restarted after AM registered . AM should re-register."; LOG.info(filteredProgress1); RMAuditLogger.logFailure(((RMApp)this.rmContext.getRMApps().get(appAttemptId.getApplicationId())).getUser(), "App Master Heartbeats", "", "ApplicationMasterService", filteredProgress1, applicationId, appAttemptId); throw new ApplicationMasterNotRegisteredException(filteredProgress1); &#125; else if(request.getResponseId() + 1 == lastResponse.getResponseId()) &#123; return lastResponse; &#125; else if(request.getResponseId() + 1 &lt; lastResponse.getResponseId()) &#123; filteredProgress1 = "Invalid responseId in AllocateRequest from application attempt: " + appAttemptId + ", expect responseId to be " + (lastResponse.getResponseId() + 1); throw new InvalidApplicationMasterRequestException(filteredProgress1); &#125; else &#123; float filteredProgress = request.getProgress(); if(!Float.isNaN(filteredProgress) &amp;&amp; filteredProgress != -1.0F / 0.0 &amp;&amp; filteredProgress &gt;= 0.0F) &#123; if(filteredProgress &gt; 1.0F || filteredProgress == 1.0F / 0.0) &#123; request.setProgress(1.0F); &#125; &#125; else &#123; request.setProgress(0.0F); &#125; this.rmContext.getDispatcher().getEventHandler().handle(new RMAppAttemptStatusupdateEvent(appAttemptId, request.getProgress())); List ask = request.getAskList(); List release = request.getReleaseList(); ResourceBlacklistRequest blacklistRequest = request.getResourceBlacklistRequest(); List blacklistAdditions = blacklistRequest != null?blacklistRequest.getBlacklistAdditions():Collections.EMPTY_LIST; List blacklistRemovals = blacklistRequest != null?blacklistRequest.getBlacklistRemovals():Collections.EMPTY_LIST; RMApp app = (RMApp)this.rmContext.getRMApps().get(applicationId); ApplicationSubmissionContext asc = app.getApplicationSubmissionContext(); Iterator allocation = ask.iterator(); while(allocation.hasNext()) &#123; ResourceRequest appAttempt = (ResourceRequest)allocation.next(); if(null == appAttempt.getNodeLabelExpression() &amp;&amp; "*".equals(appAttempt.getResourceName())) &#123; appAttempt.setNodeLabelExpression(asc.getNodeLabelExpression()); &#125; &#125; try &#123; RMServerUtils.normalizeAndValidateRequests(ask, this.rScheduler.getMaximumResourceCapability(), app.getQueue(), this.rScheduler, this.rmContext); &#125; catch (InvalidResourceRequestException var31) &#123; LOG.warn("Invalid resource ask by application " + appAttemptId, var31); throw var31; &#125; try &#123; RMServerUtils.validateBlacklistRequest(blacklistRequest); &#125; catch (InvalidResourceBlacklistRequestException var30) &#123; LOG.warn("Invalid blacklist request by application " + appAttemptId, var30); throw var30; &#125; if(!app.getApplicationSubmissionContext().getKeepContainersAcrossApplicationAttempts()) &#123; try &#123; RMServerUtils.validateContainerReleaseRequest(release, appAttemptId); &#125; catch (InvalidContainerReleaseException var29) &#123; LOG.warn("Invalid container release by application " + appAttemptId, var29); throw var29; &#125; &#125; Allocation allocation1 = this.rScheduler.allocate(appAttemptId, ask, release, blacklistAdditions, blacklistRemovals); if(!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) &#123; LOG.info("blacklist are updated in Scheduler.blacklistAdditions: " + blacklistAdditions + ", " + "blacklistRemovals: " + blacklistRemovals); &#125; RMAppAttempt appAttempt1 = app.getRMAppAttempt(appAttemptId); AllocateResponse allocateResponse = (AllocateResponse)this.recordFactory.newRecordInstance(AllocateResponse.class); if(!allocation1.getContainers().isEmpty()) &#123; allocateResponse.setNMTokens(allocation1.getNMTokens()); &#125; ArrayList updatedNodes = new ArrayList(); if(app.pullRMNodeUpdates(updatedNodes) &gt; 0) &#123; ArrayList nextMasterKey = new ArrayList(); Iterator appAttemptImpl = updatedNodes.iterator(); while(appAttemptImpl.hasNext()) &#123; RMNode amrmToken = (RMNode)appAttemptImpl.next(); SchedulerNodeReport schedulerNodeReport = this.rScheduler.getNodeReport(amrmToken.getNodeID()); Resource used = BuilderUtils.newResource(0, 0); int numContainers = 0; if(schedulerNodeReport != null) &#123; used = schedulerNodeReport.getUsedResource(); numContainers = schedulerNodeReport.getNumContainers(); &#125; NodeId nodeId = amrmToken.getNodeID(); NodeReport report = BuilderUtils.newNodeReport(nodeId, amrmToken.getState(), amrmToken.getHttpAddress(), amrmToken.getRackName(), used, amrmToken.getTotalCapability(), numContainers, amrmToken.getHealthReport(), amrmToken.getLastHealthReportTime(), amrmToken.getNodeLabels()); nextMasterKey.add(report); &#125; allocateResponse.setUpdatedNodes(nextMasterKey); &#125; allocateResponse.setAllocatedContainers(allocation1.getContainers()); allocateResponse.setCompletedContainersStatuses(appAttempt1.pullJustFinishedContainers()); allocateResponse.setResponseId(lastResponse.getResponseId() + 1); allocateResponse.setAvailableResources(allocation1.getResourceLimit()); allocateResponse.setNumClusterNodes(this.rScheduler.getNumClusterNodes()); allocateResponse.setPreemptionMessage(this.generatePreemptionMessage(allocation1)); MasterKeyData nextMasterKey1 = this.rmContext.getAMRMTokenSecretManager().getNextMasterKeyData(); if(nextMasterKey1 != null &amp;&amp; nextMasterKey1.getMasterKey().getKeyId() != amrmTokenIdentifier.getKeyId()) &#123; RMAppAttemptImpl appAttemptImpl1 = (RMAppAttemptImpl)appAttempt1; Token amrmToken1 = appAttempt1.getAMRMToken(); if(nextMasterKey1.getMasterKey().getKeyId() != appAttemptImpl1.getAMRMTokenKeyId()) &#123; LOG.info("The AMRMToken has been rolled-over. Send new AMRMToken back to application: " + applicationId); amrmToken1 = this.rmContext.getAMRMTokenSecretManager().createAndGetAMRMToken(appAttemptId); appAttemptImpl1.setAMRMToken(amrmToken1); &#125; allocateResponse.setAMRMToken(org.apache.hadoop.yarn.api.records.Token.newInstance(amrmToken1.getIdentifier(), amrmToken1.getKind().toString(), amrmToken1.getPassword(), amrmToken1.getService().toString())); &#125; lock.setAllocateResponse(allocateResponse); return allocateResponse; &#125; &#125; &#125; &#125; 总结Yarn-Cluster模式 客户端操作： SparkSubmit中根据yarnConf来初始化yarnClient，并启动yarnClient 创建客户端Application，并获取Application的ID，进一步判断集群中的资源是否满足executor和ApplicationMaster申请的资源，如果不满足则抛出IllegalArgumentException； 设置资源、环境变量：其中包括了设置Application的Staging目录、准备本地资源（jar文件、log4j.properties）、设置Application其中的环境变量、创建Container启动的Context等； 设置Application提交的Context，包括设置应用的名字、队列、AM的申请的Container、标记该作业的类型为spark； 申请Memory，最终通过submitApplication方法向ResourceManager提交该Application。当作业提交到YARN上之后，客户端就没事了，会关闭此进程，因为整个作业运行在YARN集群上进行，运行的结果将会保存到HDFS或者日志中。 Yarn操作： 运行ApplicationMaster的run方法； 设置好相关的环境变量； 创建amClient，并启动； 在Spark UI启动之前设置Spark UI的AmIpFilter； 在startUserClass函数专门启动了一个线程（名称为Driver的线程）来启动用户提交的Application，也就是启动了Driver。在Driver中将会初始化SparkContext； 等待SparkContext初始化完成，最多等待spark.yarn.applicationMaster.waitTries次数（默认为10），如果等待了的次数超过了配置的，程序将会退出；否则用SparkContext初始化yarnAllocator； 6.1. 怎么知道SparkContext初始化完成? 其实在5步骤中启动Application的过程中会初始化SparkContext，在初始化SparkContext的时候将会创建YarnClusterScheduler，在SparkContext初始化完成的时候，会调用YarnClusterScheduler类中的postStartHook方法，而该方法会通知ApplicationMaster已经初始化好了SparkContext 6.2. 为何要等待SparkContext初始化完成？ CoarseGrainedExecutorBackend启动后需要向CoarseGrainedSchedulerBackend注册 当SparkContext初始化完成的时候，通过amClient向ResourceManager注册ApplicationMaster 分配并启动Executeors。在启动Executeors之前，先要通过yarnAllocator获取到numExecutors个Container，然后在Container中启动Executeors。如果在启动Executeors的过程中失败的次数达到了maxNumExecutorFailures的次数，那么这个Application将失败，将Application Status标明为FAILED，并将关闭SparkContext。其实，启动Executeors是通过ExecutorRunnable实现的，而ExecutorRunnable内部是启动CoarseGrainedExecutorBackend的，CoarseGrainedExecutorBackend启动后会向SchedulerBackend注册。 (resourceManager是如何决定该分配几个container？ 在shell提交时跟参数 默认启动两个executor) 最后，Task将在CoarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成。 Client模式: 客户端操作： 通过SparkSubmit类的launch的函数直接调用作业的main函数（通过反射机制实现），如果是集群模式就会调用Client的main函数。 而应用程序的main函数一定都有个SparkContent，并对其进行初始化； 在SparkContent初始化中将会依次做如下的事情：设置相关的配置、注册MapOutputTracker、BlockManagerMaster、BlockManager，创建taskScheduler和dagScheduler；其中比较重要的是创建taskScheduler和dagScheduler。在创建taskScheduler的时候会根据我们传进来的master来选择Scheduler和SchedulerBackend。由于我们选择的是yarn-client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend，并将YarnClientSchedulerBackend的实例初始化YarnClientClusterScheduler，上面两个实例的获取都是通过反射机制实现的，YarnClientSchedulerBackend类是CoarseGrainedSchedulerBackend类的子类，YarnClientClusterScheduler是TaskSchedulerImpl的子类，仅仅重写了TaskSchedulerImpl中的getRackForHost方法。 初始化完taskScheduler后，将创建dagScheduler，然后通过taskScheduler.start()启动taskScheduler，而在taskScheduler启动的过程中也会调用SchedulerBackend的start方法。在SchedulerBackend启动的过程中将会初始化一些参数，封装在ClientArguments中，并将封装好的ClientArguments传进Client类中，并client.submitApplication()方法获取Application ID。 Yarn操作： 运行ApplicationMaster的run方法（runExecutorLauncher）； 无需等待SparkContext初始化完成（因为YarnClientClusterScheduler已启动完成），向sparkYarnAM注册该Application 分配Executors，这里面的分配逻辑和yarn-cluster里面类似，就不再说了。 最后，Task将在CoarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成。 在作业运行的时候，YarnClientSchedulerBackend会每隔1秒通过client获取到作业的运行状况，并打印出相应的运行信息，当Application的状态是FINISHED、FAILED和KILLED中的一种，那么程序将退出等待。 最后有个线程会再次确认Application的状态，当Application的状态是FINISHED、FAILED和KILLED中的一种，程序就运行完成，并停止SparkContext。整个过程就结束了。 Yarn的ApplicationMaster管理 client向RM提交程序(包含AM程序， AM启动命令，用户程序)； RM向资源调度器去申请资源，一旦申请的AM需要的资源，AM Laucher 便与对应的NodeManager联系启动 AM同时向AM LivenessMonitor添加进监控列表，启动对AM的监控 AM启动后，向AM Service注册报告自己的端口号，ip，track url等,之后AM会定期向AM Service发送心跳，执行allocate，AM Service会向AM LivenessMonitor更新AM的心跳时间 当用户程序执行完毕，AM向AM Service报告完成，AM Service通知AM LivenessMonitor从监控列表中删除AM，释放资源。 Reference: Spark on Yarn 任务提交流程源码分析 Yarn的ApplicationMaster管理 Hadoop Yarn详解]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Graphx]]></title>
    <url>%2F2016%2F09%2F25%2Fspark-graphx%2F</url>
    <content type="text"><![CDATA[图性能优化笔记1原始方法对求社区对应顶点数过程中，使用了单机函数collectAsMap，当社区数非常多时，该函数将数据汇总到driver节点，导致driver节点发生OOM。解决办法：避免对大数据量使用collectAsMap操作，改为RDD集合求将交集。 1234567891011121314151617181920212223242526272829def getGraphWithCommVertexNum[ED: ClassTag](g: Graph[VertexAttr, ED]): Graph[VertexAttr, ED] = &#123; /** * modified by cdemishangian * * BugFix: avoid using collectAsMap lead to driver OOM * * val cvnMap = g.vertices.map ( e =&gt; (e._2.community, 1)).reduceByKey(_ + _).collectAsMap * val graph = g.mapVertices&#123; (vid, vertexAttr) =&gt; * vertexAttr.commVertexNum = cvnMap.getOrElse(vertexAttr.community, LocalConstants.countMissingVal) * vertexAttr * &#125; */ val cvnMapRDD = g.vertices.map(e =&gt; (e._2.community, 1)).reduceByKey(_ + _) val commCntRDD = g.vertices.map(r =&gt;&#123; (r._2.community,r._1) &#125;).leftOuterJoin(cvnMapRDD).map(r=&gt; (r._2._1,(r._1,r._2._2.getOrElse(LocalConstants.countMissingVal))) ) val graph = g.subgraph(vpred = (vid,attr)=&gt; vid != LocalConstants.vidLongMissingVal).joinVertices(commCntRDD)&#123; (vid, vertexAttr, commCnt) =&gt; vertexAttr.commVertexNum = commCnt._2 vertexAttr &#125; graph &#125; 图性能优化笔记2由于spark2.0中彻底废弃了mapReduceTriplets函数，改为了aggregateMessages操作，以下代码为mapReduceTriplets转换为aggregateMessages的转换操作 1234567891011121314151617181920212223242526 val nodeWeightMapFunc = (e:EdgeTriplet[VD,Long]) =&gt; Iterator((e.srcId,e.attr), (e.dstId,e.attr)) val nodeWeightReduceFunc = (e1:Long,e2:Long) =&gt; e1+e2 val nodeWeights = graph.mapReduceTriplets(nodeWeightMapFunc,nodeWeightReduceFunc) val nodeWeightMapFunc = (e:EdgeContext[VD,Long,Long]) =&gt; &#123; e.sendToDst(e.attr) e.sendToSrc(e.attr) &#125; val nodeWeightReduceFunc = (e1:Long,e2:Long) =&gt; e1+e2 val nodeWeights = graph.aggregateMessages(nodeWeightMapFunc,nodeWeightReduceFunc) /** * Creates the messages passed between each vertex to convey neighborhood community data. */ private def sendMsg(et:EdgeTriplet[VertexState,Long]) = &#123; val m1 = (et.dstId,Map((et.srcAttr.community,et.srcAttr.communitySigmaTot)-&gt;et.attr)) val m2 = (et.srcId,Map((et.dstAttr.community,et.dstAttr.communitySigmaTot)-&gt;et.attr)) Iterator(m1, m2) &#125; private def sendMsg(et:EdgeContext[VertexState,Long,Map[(Long,Long),Long]]) = &#123; et.sendToDst(Map((et.srcAttr.community, et.srcAttr.communitySigmaTot) -&gt; et.attr)) et.sendToSrc(Map((et.dstAttr.community, et.dstAttr.communitySigmaTot) -&gt; et.attr)) &#125; 图性能优化笔记3现在业务场景在计算graphx中的三角计数指标时，由于关系数很多，达到10亿条边，通过观察运行日志发现，每次运行tcNum指标时，会产生25倍于边内存量的Shuffle Read/Write；由于缺乏对底层原理的掌握，一直不知道如何优化，经过几周的间断性尝试，今天终于通过设置图分区策略解决问题:设置PartitionStrategy.EdgePartition2D123456789101112def getGraphWithAttrTCNum[ED: ClassTag](g: Graph[VertexAttr, ED]): Graph[VertexAttr, ED] = &#123; val newGraph = g.convertToCanonicalEdges().partitionBy(PartitionStrategy.EdgePartition2D) val tcv = newGraph.triangleCount().vertices logWarning("JRDM:"+tcv.count) val graph = g.outerJoinVertices(tcv) &#123; (id, a, o) =&gt; a.tcNum = o.getOrElse(LocalConstants.countMissingVal) a &#125; graph &#125; val lineArray = line.split(“\s+”)//\s表示空格,回车,换行等空白符, +号表示一个或多个 流程 sc.textFile 读文件，生成原始的RDD 每个分区(的计算节点)把每条记录放进 PrimitiveVector 里，这个结构是spark里为primitive数据优化的存储结构。把 PrimitiveVector 里的数据一条条取出，转化成 EdgePartition ，即 EdgeRDD 的分区实现。这个过程中生成了面向列存的结构：src点的array，dst点的array，edge的属性array，以及两个正反向map(用于对应点的local id和global id)。 对 EdgeRDD 做一次count触发这次边建模任务，真正persist起来。 用 EdgePartition 去生成一个 RoutingTablePartition ，里面是vertexId到partitionId的对应关系，借助 RoutingTablePartition 生成 VertexRDD 。 由 EdgeRDD 和 VertexRDD 生成 Graph。前者维护了边的属性、边两头顶点的属性、两头顶点各自的global vertexID、两头顶点各自的local Id（在一个edge分区里的array index）、用于寻址array的正反向map。后者维护了点存在于哪个边的分区上的Map。 我们对 Fast Unfolding 算法做一个简要介绍，它分为以下两个阶段： 第一个阶段：首先将每个节点指定到唯一的一个社区，然后按顺序将节点在这些社区间进行移动。怎么移动呢？以上图中的节点 i 为例，它有三个邻居节点 j1, j2, j3，我们分别尝试将节点 i 移动到 j1, j2, j3 所在的社区，并计算相应的 modularity 变化值，哪个变化值最大就将节点 i 移动到相应的社区中去（当然，这里我们要求最大的 modularity 变化值要为正，如果变化值均为负，则节点 i 保持不动）。按照这个方法反复迭代，直到网络中任何节点的移动都不能再改善总的 modularity 值为止。 第二个阶段：将第一个阶段得到的社区视为新的“节点”（一个社区对应一个），重新构造子图，两个新“节点”之间边的权值为相应两个社区之间各边的权值的总和。 我们将上述两个阶段合起来称为一个 pass，显然，这个 pass 可以继续下去。 从上述描述我们可以看出，这种算法包含了一种 hierarchy 结构，正如对一个学校的所有初中生进行聚合一样，首先我们可以将他们按照班级来聚合，进一步还可以在此基础上按照年级来聚合，两次聚合都可以看做是一个社区发现结果，就看你想要聚合到什么层次与程度。 标签传播算法（LPA）的做法比较简单：第一步: 为所有节点指定一个唯一的标签；第二步: 逐轮刷新所有节点的标签，直到达到收敛要求为止。对于每一轮刷新，节点标签刷新的规则如下: 对于某一个节点，考察其所有邻居节点的标签，并进行统计，将出现个数最多的那个标签赋给当前节点。当个数最多的标签不唯一时，随机选一个。 注：算法中的记号 N_n^k 表示节点 n 的邻居中标签为 k 的所有节点构成的集合。 并行化问题及解决策略 进行并行化处理时，我们主要遇到两个问题：一是中间计算量过大，二是消息滞后。 中间计算量过大如果直接使用公式（1）进行Modularity计算，会导致中间计算量过大，因为它需要考虑两两节点对的情况（pairwise），即n平方的量级（n为节点个数），在大数据量情况下并不可行。 尝试的一个解决方法是，进行分步计算，如根据节点Id的hash值将数据划分成100个分区，每次只对分区内的节点进行计算。但是这种方法处理不直观，效率也不高。 经过反复尝试后，我们发现，更好的解决方法是使用化简后的公式（2）进行处理，避免了pairwise的过程。 消息滞后由于在并行化处理时，在t轮时每个节点根据t-1轮时的邻居社区信息进行更新，存在一定的消息滞后现象，会造成 “互换社区” 的问题 每个节点被分配到不同的社区中（节点1属于G1，节点2属于G2，节点3属于G3，节点4属于G4）第二轮b图时，每个节点根据它邻居的信息进行更新（如节点1的新社区为邻居节点2在第一轮的社区G2）最终情况会导致不相连的节点反而归属同一社区（如节点1与3均受到节点2的影响，归属社区G2）第三轮c图类似，造成社区的互换。造成这种情况的原因在于，每个节点根据它的邻居前一轮的信息进行变化，而它的邻居也在同步改变。 类似的，还会存在有 “社区归属延迟” 问题。示意图如图4所示。节点1的归属社区受到节点2的影响，归属到社区2。但是节点2的社区也在同步变化，它可能归属于社区3，这样就造成只有节点1归属到社区2，成为一个孤立的点。 考虑有以下两种解决策略： 添加随机值，即每轮迭代中会有部分节点的社区保持不变。如果阈值足够高，其实相当于逐个节点进行社区信息的更新，也即与串行的方法等价。使用随机值带来的问题是不能保证结果，得到的Modularity值有时高，有时低。并且，“互换社区”的问题不一定能解决。考虑到的一种解决思路是，多次运行，取最优。但是，这种方法也不太可靠，随机性较大。 得到结果后构建逻辑图，求解连通区域，将同一个连通区域的点都归为一个社区。比如初始结果是互换社区的,（格式为&lt;节点Id，归属社区&gt;），求连通区域就可以将它们都归属同一社区。这种思路也可以解决 “社区归属延迟”的问题，如初始结果是,,，节点1应该与归属社区2，但是节点2又归属于社区3，所以最终应该节点1,2,3都归属社区3。 对比上面两种方法，后一种策略充分考虑了图的特性，更为可取，能够保证结果的稳定性。大致代码如下： 总结 FastUnfolding算法，基于结果Modularity值的优化进行，得到的社区发现效果比较理想，对比LPA算法会更稳定。并且，FastUnfolding算法会不断合并节点构造新图，大大减少了计算量，使得大规模图数据的计算成为可能。 原始的FastUnfolding算法采用串行化的实现思路，不适合面对海量数据。实现中需要进行算法并行化，充分利用并行化框架带来的计算优势。在将传统的串行化算法改造成并行化算法的过程中时，会遇到中间计算量过大、消息滞后造成的问题，如“互换社区”和“社区归属延迟”问题。解决的思路是考虑图的特性，对结果再次求解连通图区域，并通过重置社区得到最终结果。这样既保证了算法的准确性，又保证其性能，从而能够在大规模的网络上，进行实际的生产应用。 Reference:[1]： GraphX 图数据建模和存储]]></content>
      <categories>
        <category>Spark</category>
        <category>Graphx</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Graphx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark常见问题解决方法]]></title>
    <url>%2F2016%2F09%2F24%2Fspark-common-problems-solutions%2F</url>
    <content type="text"><![CDATA[对于Spark程序优化，应从如下几点进行：&nbsp;&nbsp;1. 通过监控CPU、内存、网络、IO、GC、应用指标等数据，切实找到系统的瓶颈点。&nbsp;&nbsp;2. 统筹全局，制定相应的解决方案，解决问题的思路是否清晰准确很重要，切勿『头疼医头，脚疼医脚』，应总体考虑把握。&nbsp;&nbsp;3. 了解一些技术的背景知识，对于每次优化尽量做得彻底些，多进行总结。&nbsp;&nbsp;4. 程序优化通常从Stage/Cache/Partition、资源、内存/GC的优化。 1. spark运行时报错：Shutdown hook called before final status was reported.解决方法：程序存在错误，将日志down下来查看具体原因！down日志命令：yarn logs -applicationId app_id 2. Spark性能优化的9大问题及其解决方案http://book.51cto.com/art/201409/453045.htmSpark程序优化所需要关注的几个关键点——最主要的是数据序列化和内存优化 *问题1：reduce task数目不合适解决方法：需根据实际情况调节默认配置，调整方式是修改参数spark.default.parallelism。通常，reduce数目设置为core数目的2到3倍。数量太大，造成很多小任务，增加启动任务的开销；数目太少，*任务运行缓慢。 *问题2：shuffle磁盘IO时间长解决方法：设置spark.local.dir为多个磁盘，并设置磁盘为IO速度快的磁盘，通过增加IO来优化shuffle性能； *问题3：map|reduce数量大，造成shuffle小文件数目多解决方法：默认情况下shuffle文件数目为map tasks * reduce tasks. 通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目； *问题4：序列化时间长、结果大解决方法：Spark默认使.用JDK.自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KryoSerializer。另外如果结果已经很大，可以使用广播变量； *问题5：单条记录消耗大解决方法：使用mapPartition替换map，mapPartition是对每个Partition进行计算，而map是对partition中的每条记录进行计算； *问题6: collect输出大量结果时速度慢解决方式：collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式?文件系统，然后查看文件系统中的内容； *问题7: 任务执行速度倾斜解决方式：如果是数据倾斜，一般是partition key取的不好，可以考虑其它的并行处理方式 ，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉； *问题8: 通过多步骤的RDD操作后有很多空任务或者小任务产生解决方式：使用coalesce或repartition去减少RDD中partition数量； *问题9：Spark Streaming吞吐量不高解决方式：可以设置spark.streaming.concurrentJobs 3. intellij idea直接编译spark源码及问题解决:* http://blog.csdn.net/tanglizhe1105/article/details/50530104* http://stackoverflow.com/questions/18920334/output-path-is-shared-between-the-same-module-error12Spark编译：clean package -Dmaven.test.skip=true参数：-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m 4. import Spark source code into intellj, build Error: not found: type SparkFlumeProtocol and EventBatchhttp://stackoverflow.com/questions/33311794/import-spark-source-code-into-intellj-build-error-not-found-type-sparkflumepr 5. 整理对Spark SQL的理解：http://www.aboutyun.com/thread-8575-1-1.html 6. Spark GBDT实现方式：spark gbdt的实现基于：J.H. Friedman. “Stochastic Gradient Boosting.” 1999.spark。&nbsp;&nbsp;1) gbdt使用一般的残差更新方式，利用残差（梯度方向）更新样本数据集，做为下棵树模型训练的样本1data = input.map(point =&gt; LabeledPoint(-loss.gradient(partialModel, point),point.features)) &nbsp;&nbsp;2) 建树过程用variance作为split准则, xgboost对目标函数进行变换，每轮建树的过程用gradient和hessian计算gain作为split准则&nbsp;&nbsp;3) 算法性能（效率、支持数据量）&nbsp;&nbsp;4) 算法结果的一致性 7. UDAF（User- Defined Aggregation Funcation）：http://p-x1984.iteye.com/blog/1156392Hive自定义UDF和聚合函数UDAF：http://computerdragon.blog.51cto.com/6235984/1288567Hive自定义UDF和聚合函数UDAF：http://www.tuicool.com/articles/mYZ7R3 8. java.lang.NoSuchMethodException: java.util.Set.()&#104;&#116;&#116;&#x70;&#58;&#47;&#47;&#109;&#97;&#x69;&#x6c;&#45;&#97;&#114;&#x63;&#104;&#105;&#x76;&#x65;&#115;&#46;&#97;&#112;&#x61;&#99;&#104;&#101;&#46;&#111;&#x72;&#x67;&#47;&#109;&#x6f;&#x64;&#95;&#109;&#x62;&#111;&#x78;&#47;&#x68;&#105;&#x76;&#101;&#x2d;&#x75;&#115;&#x65;&#x72;&#x2f;&#50;&#48;&#49;&#51;&#x30;&#55;&#46;&#109;&#98;&#x6f;&#120;&#x2f;&#37;&#x33;&#67;&#67;&#x45;&#49;&#67;&#65;&#x34;&#49;&#x43;&#x2e;&#49;&#49;&#x37;&#54;&#x45;&#x25;&#x32;&#53;&#114;&#x64;&#109;&#x40;&#98;&#97;&#121;&#x6e;&#111;&#116;&#x65;&#46;&#x63;&#x6f;&#x6d;&#x25;&#51;&#69; 9. Hive可以允许用户编写自己定义的函数UDF，来在查询中使用。Hive中有3种UDF：&nbsp;&nbsp;* UDF：操作单个数据行，产生单个数据行;&nbsp;&nbsp;* UDAF：操作多个数据行，产生一个数据行。&nbsp;&nbsp;* UDTF：操作一个数据行，产生多个数据行一个表作为输出。 10. Hive查询数据时，有些聚类函数在HQL没有自带，需要用户自定义实现UDTF。实现用户自定义聚合函数: Sum, Average…… n – 1&nbsp;&nbsp;10.1. 必须import org.apache.hadoop.hive.ql.exec.UDAF和org.apache.hadoop.hive.ql.exec.UDAFEvaluator。&nbsp;&nbsp;10.2. 函数类需要继承UDAF类，内部类Evaluator实UDAFEvaluator接口。&nbsp;&nbsp;10.3. Evaluator需要实现init、iterate、terminatePartial、merge、terminate这几个函数。&nbsp;&nbsp;10.3.1. init函数实现接口UDAFEvaluator的init函数。&nbsp;&nbsp;10.3.2. iterate接收传入的参数，并进行内部的轮转。其返回类型为boolean。&nbsp;&nbsp;10.3.3. terminatePartial无参数，其为iterate函数轮转结束后，返回轮转数据，terminatePartial类于hadoop的Combiner。&nbsp;&nbsp;10.3.4. merge接收terminatePartial的返回结果，进行数据merge操作，其返回类型为boolean。&nbsp;&nbsp;10.3.5. terminate返回最终的聚集函数结果。 11. Apache Zeppelin编译安装：http://www.iteblog.com/archives/1573Apache Zeppelin installation grunt build error：http://stackoverflow.com/questions/33352309/apache-zeppelin-installation-grunt-build-error?rq=1解决方案：进入web模块npm install 12. Spark源码编译遇到的问题解决：http://www.tuicool.com/articles/NBVvai内存不够，这个错误是因为编译的时候内存不够导致的，可以在编译的时候加大内存。12345678910[ERROR] PermGen space -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors,re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1]http://cwiki.apache.org/confluence/display/MAVEN/OutOfMemoryErrorexport MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m" 13. Exception in thread “main” java.lang.UnsatisfiedLinkError: no jnind4j in java.library.pathI’m using a 64-Bit Java on Windows and still get the no jnind4j in java.library.path errorIt may be that you have incompatible DLLs on your PATH. In order to tell DL4J to ignore those you have to add the following as a VM parameter (Run -&gt; Edit Configurations -&gt; VM Options in IntelliJ): -Djava.library.path=&quot;&quot; 14. spark2.0本地运行源码报错解决办法：&nbsp;&nbsp;14.1. 修改对应pom中的依赖jar包，将scope级别由provided改为compile&nbsp;&nbsp;14.2. 运行类之前，去掉make选项；在运行vm设置中增加-Dspark.master=local&nbsp;&nbsp;14.3. Win7下运行spark example代码报错： java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:D:/SourceCode/spark-2.0.0/spark-warehouse`修改SQLConf类中WAREHOUSE_PATH变量，将file:前缀改为file:/或file:/// createWithDefault(&quot;file:/${system:user.dir}/spark-warehouse&quot;) &nbsp;&nbsp;14.4. local模式运行：-Dspark.master=local 15. Spark底层运行原理疑问？&nbsp;&nbsp;* SparkSession、SparkContext、SQLContext、HiveContext之间的关系与实现机制？&nbsp;&nbsp;* RDD、DataFrame、DataSet之间的区别与联系，实现原理？&nbsp;&nbsp;* Spark执行原理？&nbsp;&nbsp;* Spark SQL执行原理？&nbsp;&nbsp;* Spark中的常见设计模式？&nbsp;&nbsp;* Spark主要包括 调度与任务分配、I/O模块、通信控制模块、容错模块、Shuffle模块。&nbsp;&nbsp;* Spark 按照 ①应用 application ②作业 job ③ stage ④ task 四个层次进行调度，采用经典的FIFO和FAIR等调度算法。 16. 解决Task not serializable Exception错误方法1：将RDD中的所有数据通过JDBC连接写入数据库，若使用map函数，可能要为每个元素都创建connection，这样开销很大，如果使用mapPartitions，那么只需要针对每个分区建立connection；mapPartitions处理后返回的是Iterator。方法2：对未序列化的对象加@transisent引用，在进行网络通信时不对对象中的属性进行序列化 17. 使用LZO过程会发现它有两种压缩编码可以使用，即LzoCodec和LzopCodec，下面说说它们区别：&nbsp;&nbsp;17.1. LzoCodec比LzopCodec更快， LzopCodec为了兼容LZOP程序添加了如 bytes signature, header等信息&nbsp;&nbsp;17.2. 如果使用 LzoCodec作为Reduce输出，则输出文件扩展名为”.lzo_deflate”，它无法被lzop读取；如果使用LzopCodec作为Reduce输出，则扩展名为”.lzo”，它可以被lzop读取&nbsp;&nbsp;17.3. 生成lzo index job的”DistributedLzoIndexer“无法为 LzoCodec，即 “.lzo_deflate”扩展名的文件创建index&nbsp;&nbsp;17.4. ”.lzo_deflate“文件无法作为MapReduce输入，”.LZO”文件则可以。&nbsp;&nbsp;17.5. 综上所述得出最佳实践：map输出的中间数据使用 LzoCodec，reduce输出使用 LzopCodec 18. JVM线程池发展趋势：http://www.importnew.com/15082.html对于传统线程池机制，一个强大的替代方案就是基于事件模型。这种基于事件的线程轮询/线程池/线程调度机制在函数式编程中很常见。关于这个概念的一个非常流行的实现是基于actor的系统（译者注：Scala的并发系统），Akka已成为其实际上的标准。（译者注：Akka，一种善于处理进程间通信的框架） 19. 这个函数在func(“11”)调用时候正常,但是在执行func(11)或func(1.1)时候就会报error: type mismatch的错误.* 针对特定的参数类型, 重载多个func函数,这个不难, 传统JAVA中的思路, 但是需要定义多个函数* 使用超类型, 比如使用AnyVal,Any;这样的话比较麻烦,需要在函数中针对特定的逻辑做类型转化,从而进一步处理上面两个方法使用的是传统JAVA思路,虽然都可以解决该问题,但是缺点是不够简洁;在充满了语法糖的Scala中,针对类型转换提供了特有的implicit隐式转化的功能; 20. org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle解决方案：这种问题一般发生在有大量shuffle操作的时候,task不断的failed,然后又重执行，一直循环下去，直到application失败。一般遇到这种问题提高executor内存即可,同时增加每个executor的cpu,这样不会减少task并行度。 21. Spark ML PipeLine GBT/RF预测时报错，java.util.NoSuchElementException: key not found: 8.0错误原因：由于GBT/RF模型输入setFeaturesCol，setLabelCol参数列名不一致导致。解决方案：只保存训练算法模型，不保存PipeLineModel 22. linux删除乱码文件，find . -inum 54263996 -exec rm {} -rf \; 23. org.apache.spark.SparkException: Exception thrown in awaitResultset &quot;spark.sql.broadcastTimeout&quot; to increase the timeout 24. Caused by: java.lang.RuntimeException: Failed to commit taskCaused by: org.apache.spark.executor.CommitDeniedException: attempt_201603251514_0218_m_000245_0: Not committed because the driver did not authorize commit如果你比较了解spark中的stage是如何划分的，这个问题就比较简单了。一个Stage中包含的task过大，一般由于你的transform过程太长，因此driver给executor分发的task就会变的很大。所以解决这个问题我们可以通过拆分stage解决。也就是在执行过程中调用cache.count缓存一些中间数据从而切断过长的stage。 25. Spark Streaming性能调优：https://www.iteblog.com/archives/1333优化运行时间| 增加并行度 确保使用整个集群的资源，而不是把任务集中在几个特定的节点上。对于包含shuffle的操作，增加其并行度以确保更为充分地使用集群资源；| 减少数据序列化 反序列化的负担 Spark Streaming默认将接受到的数据序列化后存储，以减少内存的使用。但是序列化和反序列话需要更多的CPU时间，因此更加高效的序列化方式（Kryo）和自定义的系列化接口可以更高效地使用CPU；| 设置合理的batch duration（批处理时间间隔） 在Spark Streaming中，Spark会每隔batchDuration间隔时间提交一次job，Job之间有可能存在依赖关系，后面的Job必须确保前面的作业执行结束后才能提交。若前面的Job执行的时间超出了批处理时间间隔，那么后面的Job就无法按时提交，这样就会进一步拖延接下来的Job，造成后续Job的阻塞。因此设置一个合理的批处理间隔以确保作业能够在这个批处理间隔内结束时必须的；| 减少因任务提交和分发所带来的负担 通常情况下，Akka框架能够高效地确保任务及时分发，但是当批处理间隔非常小（500ms）时，提交和分发任务的延迟就变得不可接受了。使用Standalone和Coarse-grained Mesos模式通常会比使用Fine-grained Mesos模式有更小的延迟。| 缓存需要经常使用的数据 调用rdd.cache()来缓存数据，加快数据处理 优化内存使用| 控制batch size（批处理间隔内的数据量） Spark Streaming会把批处理间隔内接收到的所有数据存放在Spark内部的可用内存区域中，因此必须确保当前节点Spark的可用内存中少能容纳这个批处理时间间隔内的所有数据，否则必须增加新的资源以提高集群的处理能力；| 及时清理不再使用的数据 前面讲到Spark Streaming会将接受的数据全部存储到内部可用内存区域中，因此对于处理过的不再需要的数据应及时清理，以确保Spark Streaming有富余的可用内存空间。通过设置合理的spark.cleaner.ttl【Deprecated】时长来及时清理超时的无用数据，这个参数需要小心设置以免后续操作中所需要的数据被超时错误处理，还可以配置选项spark.streaming.unpersist=true来更智能的持久化(unpersist)RDD，该配置使系统找出那些不需要经常保存的RDD，然后去持久化它们，这样可以减少spark rdd的内存使用，还可以改善垃圾回收行为；| 观察及适当调整GC策略 GC会影响Job的正常运行，可能延长Job的执行时间，引起一系列不可预料的问题。观察GC的运行情况，采用不同的GC策略以进一步减小内存回收对Job运行的影响。| 设备合理的CPU资源数]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 学习笔记]]></title>
    <url>%2F2016%2F09%2F11%2Fspark-notices%2F</url>
    <content type="text"><![CDATA[以下是在学习和使用spark过程中遇到的一些问题，记录下来。 1. 首先来说说spark任务运行完后查错最常用的一个命令，那就是把任务运行日志down下来。 程序存在错误，将日志down下来查看具体原因！down日志命令：yarn logs -applicationId app_id 2. Spark性能优化的9大问题及其解决方案：http://book.51cto.com/art/201409/453045.htmSpark程序优化所需要关注的几个关键点——最主要的是数据序列化和内存优化 *问题1：reduce task数目不合适解决方法：需根据实际情况调节默认配置，调整方式是修改参数spark.default.parallelism。通常，reduce数目设置为core数目的2到3倍。数量太大，造成很多小任务，增加启动任务的开销；数目太少，*任务运行缓慢。 *问题2：shuffle磁盘IO时间长解决方法：设置spark.local.dir为多个磁盘，并设置磁盘为IO速度快的磁盘，通过增加IO来优化shuffle性能； *问题3：map|reduce数量大，造成shuffle小文件数目多解决方法：默认情况下shuffle文件数目为map tasks * reduce tasks. 通过设置spark.shuffle.consolidateFiles为true，来合并shuffle中间文件，此时文件数为reduce tasks数目； *问题4：序列化时间长、结果大解决方法：Spark默认使.用JDK.自带的ObjectOutputStream，这种方式产生的结果大、CPU处理时间长，可以通过设置spark.serializer为org.apache.spark.serializer.KryoSerializer。另外如果结果已经很大，可以使用广播变量；*问题5：单条记录消耗大解决方法：使用mapPartition替换map，mapPartition是对每个Partition进行计算，而map是对partition中的每条记录进行计算； *问题6：collect输出大量结果时速度慢解决方法：collect源码中是把所有的结果以一个Array的方式放在内存中，可以直接输出到分布式?文件系统，然后查看文件系统中的内容； *问题7：任务执行速度倾斜解决方法：如果是数据倾斜，一般是partition key取的不好，可以考虑其它的并行处理方式 ，并在中间加上aggregation操作；如果是Worker倾斜，例如在某些worker上的executor执行缓慢，可以通过设置spark.speculation=true 把那些持续慢的节点去掉； *问题8：通过多步骤的RDD操作后有很多空任务或者小任务产生解决方法：使用coalesce或repartition去减少RDD中partition数量 *问题9：Spark Streaming吞吐量不高解决方法：可以设置spark.streaming.concurrentJobs 3. intellij idea直接编译spark源码及问题解决:* http://blog.csdn.net/tanglizhe1105/article/details/50530104* http://stackoverflow.com/questions/18920334/output-path-is-shared-between-the-same-module-error12Spark编译：clean package -Dmaven.test.skip=true参数：-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m 4. import Spark source code into intellj, build Error: not found: type SparkFlumeProtocol and EventBatchhttp://stackoverflow.com/questions/33311794/import-spark-source-code-into-intellj-build-error-not-found-type-sparkflumepr 5. java.lang.NoSuchMethodException: java.util.Set.()&#104;&#x74;&#x74;&#112;&#x3a;&#47;&#x2f;&#109;&#97;&#x69;&#x6c;&#45;&#x61;&#x72;&#x63;&#104;&#x69;&#118;&#101;&#115;&#46;&#x61;&#112;&#97;&#99;&#104;&#x65;&#46;&#111;&#114;&#103;&#47;&#x6d;&#x6f;&#x64;&#95;&#x6d;&#x62;&#111;&#x78;&#x2f;&#x68;&#105;&#x76;&#x65;&#45;&#117;&#115;&#101;&#x72;&#47;&#50;&#x30;&#49;&#x33;&#x30;&#x37;&#x2e;&#109;&#x62;&#111;&#x78;&#x2f;&#x25;&#51;&#x43;&#67;&#69;&#49;&#67;&#x41;&#52;&#x31;&#67;&#x2e;&#x31;&#49;&#x37;&#54;&#69;&#x25;&#50;&#53;&#114;&#100;&#109;&#64;&#98;&#x61;&#x79;&#110;&#111;&#x74;&#101;&#46;&#x63;&#x6f;&#109;&#x25;&#51;&#69; 6. Apache Zeppelin编译安装：http://www.iteblog.com/archives/1573Apache Zeppelin installation grunt build error：http://stackoverflow.com/questions/33352309/apache-zeppelin-installation-grunt-build-error?rq=1解决方法：进入web模块npm install 7. Spark源码编译遇到的问题解决：http://www.tuicool.com/articles/NBVvai解决方法：内存不够，这个错误是因为编译的时候内存不够导致的，可以在编译的时候加大内存。12345678910[ERROR] PermGen space -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors,re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1]http://cwiki.apache.org/confluence/display/MAVEN/OutOfMemoryErrorexport MAVEN_OPTS="-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m" 8. Exception in thread “main” java.lang.UnsatisfiedLinkError: no jnind4j in java.library.pathI’m using a 64-Bit Java on Windows and still get the no jnind4j in java.library.path errorIt may be that you have incompatible DLLs on your PATH. In order to tell DL4J to ignore those you have to add the following as a VM parameter (Run -&gt; Edit Configurations -&gt; VM Options in IntelliJ): -Djava.library.path=&quot;&quot; 9. spark2.0本地运行源码报错解决办法： 1. 修改对应pom中的依赖jar包，将scope级别由provided改为compile2. 运行类之前，去掉make选项；在运行vm设置中增加-Dspark.master=local3. Win7下运行spark example代码报错：java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:D:/SourceCode/spark-2.0.0/spark-warehouse修改SQLConf类中WAREHOUSE_PATH变量，将file:前缀改为file:/或file:///createWithDefault(“file:/${system:user.dir}/spark-warehouse”)4. local模式运行：-Dspark.master=local 10. 解决Task not serializable Exception错误方法1：将RDD中的所有数据通过JDBC连接写入数据库，若使用map函数，可能要为每个元素都创建connection，这样开销很大，如果使用mapPartitions，那么只需要针对每个分区建立connection；mapPartitions处理后返回的是Iterator。方法2：对未序列化的对象加@transisent引用，在进行网络通信时不对对象中的属性进行序列化 11. 这个函数在func(“11”)调用时候正常,但是在执行func(11)或func(1.1)时候就会报error: type mismatch的错误. 这个问题很好解决 - 针对特定的参数类型, 重载多个func函数,这个不难, 传统JAVA中的思路, 但是需要定义多个函数- 使用超类型, 比如使用AnyVal,Any;这样的话比较麻烦,需要在函数中针对特定的逻辑做类型转化,从而进一步处理上面两个方法使用的是传统JAVA思路,虽然都可以解决该问题,但是缺点是不够简洁;在充满了语法糖的Scala中,针对类型转换提供了特有的implicit隐式转化的功能; 12. org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle解决方法：这种问题一般发生在有大量shuffle操作的时候,task不断的failed,然后又重执行，一直循环下去，直到application失败。一般遇到这种问题提高executor内存即可,同时增加每个executor的cpu,这样不会减少task并行度。 13. Spark ML PipeLine GBT/RF预测时报错，java.util.NoSuchElementException: key not found: 8.0错误原因：由于GBT/RF模型输入setFeaturesCol，setLabelCol参数列名不一致导致。解决方法：只保存训练算法模型，不保存PipeLineModel 14. linux删除乱码文件，find . -inum 54263996 -exec rm {} -rf \; 15. org.apache.spark.SparkException: Exception thrown in awaitResultset &quot;spark.sql.broadcastTimeout&quot; to increase the timeout 16. Caused by: java.lang.RuntimeException: Failed to commit taskCaused by: org.apache.spark.executor.CommitDeniedException: attempt_201603251514_0218_m_000245_0: Not committed because the driver did not authorize commit如果你比较了解spark中的stage是如何划分的，这个问题就比较简单了。一个Stage中包含的task过大，一般由于你的transform过程太长，因此driver给executor分发的task就会变的很大。所以解决这个问题我们可以通过拆分stage解决。也就是在执行过程中调用cache.count缓存一些中间数据从而切断过长的stage。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark资料链接汇总]]></title>
    <url>%2F2016%2F09%2F10%2Fspark-doc-index%2F</url>
    <content type="text"><![CDATA[【SparkX】 基于Spark Graphx的大规模用户图计算和应用 快刀初试：Spark GraphX在淘宝的实践 Spark中文手册9：Spark GraphX编程指南 Spark Graphx:构建graph和聚合消息 GraphX Programming Guide Spark的Graphx学习笔记–Pregel Apache Spark源码走读之14 – Graphx实现剖析 - 徽沪一郎 Spark+GraphX大规模图计算和图挖掘（V3.0）王家林 Graph analytics with Graphx 【Spark】 Spark的成功案例 Spark实时流处理编程指南 Skewed Join Solutions Skewed Join Optimization Spark Programming Guide Reza Zadeh spark ppt资料 spark mail list 一个 KCore 算法引发的 StackOverflow 奇案 数盟DataUnion Spark-tech Databricks Spark Knowledge Base 解决Task not serializable Exception错误 Spark的四种编译方法 Spark运行架构 Yarn-cluster和Yarn-client区别与联系 Spark性能优化——开发调优篇 Spark性能优化指南——基础篇 祝威廉Spark-Streaming Kafka+Spark Streaming+Redis实时计算整合实践 Spark 调优 【Spark2.0新特性】 SparkSession RDD、DataFrame和DataSet的区别 Spark RDD. DataFrame和DataSet的区别 深入理解Spark核心思想与源码分析 Spark Network 模块分析 【Machine Learning &amp; Deep Learning】 深度学习三十年创新路 如何在MLlib中实现随机森林和梯度提升树（GBTs） 深度学习-LeCun. Bengio和Hinton的联合综述 C.Olah Neural Networks (General) Blog 数据夜话：机器学习的七嘴八舌 Google DeepMind SparkR(R on Spark)编程指南 理解L-BFGS算法 【架构/分享】 大数据时代抽样的是是非非 把小样本经验用在海量样本筛选上，才是大数据的价值 大数据是否需要抽样？ 互联网金融时代下机器学习与大数据风控系统 量化派基于Hadoop. Spark. Storm的大数据风控架构 机器学习算法之旅 广点通背后的大数据技术秘密 广点通DMP定向功能技术体系解析 腾讯社交网络的大数据建模框架探索报告 Spark技术解析及其在百度最大千台单集群的应用实践 百度知识图谱中的NLP技术 【Others】 Gephi的设计理念及Gephi可视化需要什么样的数据 scala macro-使case copy易读 treehugger.scala 3 approaches to Scala code generation Scala macro annotations: a real-world example SparkTask未序列化(Tasknotserializable)问题分析 Apache Zeppelin 1 Apache Zeppelin 2 Apache Zeppelin 2 基于Apache Zeppelin Notebook和R的交互式数据科学 【Scala】Effective ScalaScala 课堂!]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pelican环境搭建&Markdown学习小记]]></title>
    <url>%2F2016%2F09%2F09%2Fpelican-notes%2F</url>
    <content type="text"><![CDATA[Pelican环境搭建笔记 将md文档编译为html,并输出到output目录命令： 1pelican content 在命令行里面切换到output目录下面，启动Pelican的本地web服务命令： 1python -m SimpleHTTPServer pelican所有插件安装均在pelicanconf.py文件中配置参数 代码块theme设置，直接修改flex theme base.html中代码块默认主题为monokai，然后再在pelicanconf.py文件中配置参数： 1MD_EXTENSIONS = ['codehilite(pygments_style=monokai,css_class=highlight,linenums=True)', 'extra'] Markdown常用语法笔记 引用：如果你需要引用一小段别处的句子，那么就要用引用的格式。只需要在文本前加入 &gt; 这种尖括号（大于号）即可 图片与链接：插入链接与插入图片的语法很像，区别在一个 !号。图片为：链接为：msjbear 粗体与斜体：Markdown的粗体和斜体也非常简单，用两个包含一段文本就是粗体的语法，用一个包含一段文本就是斜体的语法。 代码框：如果你是个程序猿，需要在文章里优雅的引用代码框，在 Markdown下实现也非常简单，只需要用两个 ` 把中间的代码包裹起来。使用 tab 键即可缩进。 分割线：分割线的语法只需要三个 * 号]]></content>
      <categories>
        <category>Notes</category>
      </categories>
      <tags>
        <tag>pelican</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>